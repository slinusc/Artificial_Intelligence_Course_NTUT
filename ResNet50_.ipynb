{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1025,
     "status": "ok",
     "timestamp": 1729661144826,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     },
     "user_tz": -480
    },
    "id": "nXNK-xCMB1kh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights, vgg16\n",
    "from PIL import Image\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1729661146380,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     },
     "user_tz": -480
    },
    "id": "iCvFpqWzD6VO"
   },
   "outputs": [],
   "source": [
    "# CIFAR10Dataset class\n",
    "class CIFAR10Dataset(Dataset):\n",
    "    def __init__(self, data_path, batch_files, transform=None, augmentation=None):\n",
    "        self.data_path = data_path\n",
    "        self.batch_files = batch_files\n",
    "        self.augmentation = augmentation  # Augmentation for specific labels\n",
    "        self.transform = transform  # Default transform for all other labels\n",
    "        self.batch_data = None\n",
    "        self.batch_labels = None\n",
    "        self.batch_index = -1\n",
    "        self.index_map = []\n",
    "        self._create_index_map()\n",
    "\n",
    "    def _create_index_map(self):\n",
    "        \"\"\"Create a map of global indices to batch indices.\"\"\"\n",
    "        for batch_num, batch_file in enumerate(self.batch_files):\n",
    "            with open(os.path.join(self.data_path, batch_file), 'rb') as f:\n",
    "                batch = pickle.load(f, encoding='bytes')\n",
    "                batch_size = len(batch[b'labels'])\n",
    "                self.index_map.extend([(batch_num, i) for i in range(batch_size)])\n",
    "\n",
    "    def _load_batch(self, batch_num):\n",
    "        \"\"\"Load a batch given its batch number.\"\"\"\n",
    "        batch_file = self.batch_files[batch_num]\n",
    "        with open(os.path.join(self.data_path, batch_file), 'rb') as f:\n",
    "            batch = pickle.load(f, encoding='bytes')\n",
    "            self.batch_data = batch[b'data'].reshape(-1, 3, 32, 32)\n",
    "            self.batch_labels = batch[b'labels']\n",
    "        self.batch_index = batch_num\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_num, in_batch_idx = self.index_map[idx]\n",
    "\n",
    "        # Load the batch\n",
    "        if batch_num != self.batch_index:\n",
    "            self._load_batch(batch_num)\n",
    "\n",
    "        # Fetch image and label from the loaded batch\n",
    "        image = self.batch_data[in_batch_idx]\n",
    "        label = self.batch_labels[in_batch_idx]\n",
    "\n",
    "        # Convert to HWC format (Height, Width, Channels)\n",
    "        image = image.transpose(1, 2, 0)\n",
    "\n",
    "        # Convert the image to a PIL image\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "        # If augmentation exists, apply augmentation first and skip the ToTensor in transform\n",
    "        if self.augmentation: # label in [0, 3, 4, 5]\n",
    "            # Apply augmentation first\n",
    "            image = self.augmentation(image)\n",
    "\n",
    "            # If the augmentation already applied ToTensor, we shouldn't apply it again.\n",
    "            if isinstance(image, torch.Tensor):\n",
    "                # Only apply other transforms that are NOT ToTensor()\n",
    "                for t in self.transform.transforms:\n",
    "                    if not isinstance(t, transforms.ToTensor):\n",
    "                        image = t(image)\n",
    "            else:\n",
    "                # If not tensor, apply the full transform pipeline\n",
    "                image = self.transform(image)\n",
    "        else:\n",
    "            # If no augmentation or other labels, apply the normal transformation\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1019,
     "status": "ok",
     "timestamp": 1729661263612,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     },
     "user_tz": -480
    },
    "id": "ykMb0srNDUn9"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(trainloader, validloader, model, criterion, optimizer, num_epochs=10, patience=5, data_path=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # To store loss and accuracy for each epoch\n",
    "    train_losses, valid_losses = [], []\n",
    "    valid_accuracies = []  # To store validation accuracy\n",
    "    best_valid_acc = 0.0  # Track best validation accuracy\n",
    "    best_valid_loss = float('inf')  # Track best validation loss for early stopping\n",
    "    epochs_no_improve = 0  # Counter for early stopping\n",
    "\n",
    "    # Construct model save path based on data_path\n",
    "    if data_path:\n",
    "        model_save_path = '/'.join(data_path.split('/')[:-3]) + \"/resnet_model_with_aug.pkl\"\n",
    "    else:\n",
    "        model_save_path = \"resnet_model_with_aug.pkl\"  # Fallback save path if data_path isn't provided\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Starting epoch {epoch+1}\")  # Debugging line\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        # Train on training data\n",
    "        progress_bar = tqdm(trainloader, desc=f'Epoch {epoch+1}/{num_epochs} (Train)')\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            total_train_samples += labels.size(0)\n",
    "\n",
    "        # Compute epoch training loss\n",
    "        epoch_train_loss = running_loss / total_train_samples\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        print(f'Training: Loss: {epoch_train_loss:.4f}')\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        running_valid_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_valid_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm(validloader, desc=f'Epoch {epoch+1}/{num_epochs} (Valid)')\n",
    "            for inputs, labels in progress_bar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                running_valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Get the predicted labels and calculate accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_valid_samples += labels.size(0)\n",
    "\n",
    "        # Compute epoch validation loss\n",
    "        epoch_valid_loss = running_valid_loss / total_valid_samples\n",
    "        valid_losses.append(epoch_valid_loss)\n",
    "        print(f'Validation: Loss: {epoch_valid_loss:.4f}')\n",
    "\n",
    "        # Compute validation accuracy\n",
    "        epoch_valid_acc = correct_predictions / total_valid_samples * 100\n",
    "        valid_accuracies.append(epoch_valid_acc)\n",
    "        print(f'Validation: Accuracy: {epoch_valid_acc:.2f}%')\n",
    "\n",
    "        # Check if the current model has the best validation accuracy so far\n",
    "        if epoch_valid_acc > best_valid_acc:\n",
    "            best_valid_acc = epoch_valid_acc\n",
    "            print(f'Saving best model at epoch {epoch+1} with validation accuracy {best_valid_acc:.2f}%')\n",
    "\n",
    "            # Save the model using pickle at the constructed path\n",
    "            with open(model_save_path, 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "\n",
    "        # Check for early stopping based on validation loss\n",
    "        if epoch_valid_loss < best_valid_loss:\n",
    "            best_valid_loss = epoch_valid_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "    # Return the training and validation losses and validation accuracies\n",
    "    return train_losses, valid_losses, valid_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1729661153143,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     },
     "user_tz": -480
    },
    "id": "GNomTbr_DcW1"
   },
   "outputs": [],
   "source": [
    "def run_training(augment=False, epochs=10):\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Path to the dataset in Google Drive\n",
    "    data_path = '/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/cifar-10-python/cifar-10-batches-py/'\n",
    "\n",
    "    # Training batch file names (excluding the test batch)\n",
    "    train_batches = [f'data_batch_{i}' for i in range(1, 6)]\n",
    "\n",
    "    # Transformation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # Augmentation\n",
    "    augmentation = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, padding=2),\n",
    "        transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.02),  # Color jitter\n",
    "    ])\n",
    "\n",
    "    if augment:\n",
    "    # Instantiate dataset with transformations (for training and validation)\n",
    "      train_dataset = CIFAR10Dataset(data_path, train_batches, transform=transform, augmentation=augmentation)\n",
    "    else:\n",
    "    # Instantiate dataset without transformations (for training and validation)\n",
    "      train_dataset = CIFAR10Dataset(data_path, train_batches, transform=transform)\n",
    "\n",
    "    # Split the training dataset into train and validation subsets\n",
    "    train_size = int(0.9 * len(train_dataset))  # Use 90% for training\n",
    "    valid_size = len(train_dataset) - train_size  # Use 10% for validation\n",
    "    train_subset, valid_subset = torch.utils.data.random_split(train_dataset, [train_size, valid_size])\n",
    "\n",
    "    # DataLoader for training and validation\n",
    "    trainloader = DataLoader(train_subset, batch_size=128, shuffle=True, num_workers=4)\n",
    "    validloader = DataLoader(valid_subset, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n",
    "    # Instantiate the model (using ResNet-50)\n",
    "    model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-2)\n",
    "\n",
    "    # Train and validate the model\n",
    "    train_losses, valid_losses, valid_accuracies = train_and_evaluate(\n",
    "        trainloader, validloader, model, criterion, optimizer, num_epochs=epochs, patience=5, data_path=data_path\n",
    "    )\n",
    "\n",
    "    return model, train_losses, valid_losses, valid_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4207628,
     "status": "ok",
     "timestamp": 1729603925297,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     },
     "user_tz": -480
    },
    "id": "gVSYaZwlG_iU",
    "outputId": "560a2071-48c3-49ad-fdd8-5421d91775ec"
   },
   "outputs": [],
   "source": [
    "# lets train the model without augmentation\n",
    "model, train_losses, valid_losses, valid_accuracies = run_training(augment=False, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 1894,
     "status": "ok",
     "timestamp": 1729669282866,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     },
     "user_tz": -480
    },
    "id": "r23UY9kjUd_X",
    "outputId": "1e89dc3e-d897-4e35-86b7-9d94ca921037"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Reading the CSV file\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/training_results_no_augmentation.csv\")\n",
    "\n",
    "# Creating a figure and axis for the plot\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plotting Training and Validation Loss on the left axis\n",
    "ax1.plot(df[\"Epoch\"], df[\"Training Loss\"], label=\"Training Loss\", color=\"blue\")\n",
    "ax1.plot(df[\"Epoch\"], df[\"Validation Loss\"], label=\"Validation Loss\", color=\"orange\")\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend(loc=\"upper left\")\n",
    "\n",
    "# Creating a second y-axis for the accuracy\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(df[\"Epoch\"], df[\"Validation Accuracy\"], label=\"Validation Accuracy\", color=\"green\")\n",
    "ax2.set_ylabel('Validation Accuracy')\n",
    "ax2.legend(loc=\"upper right\")\n",
    "\n",
    "# Displaying the plot\n",
    "plt.title('Training/Validation Loss and Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6S3CYMIqZ-f"
   },
   "source": [
    "Now check the result first on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 693,
     "status": "ok",
     "timestamp": 1729660842856,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     },
     "user_tz": -480
    },
    "id": "6_cmQEFJsSTs"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Function to test model\n",
    "def test_model_with_confusion_matrix(model, testloader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    misclassified_images = []\n",
    "    misclassified_labels = []\n",
    "    misclassified_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(enumerate(testloader), total=len(testloader), desc='Testing')\n",
    "        for i, (inputs, labels) in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Append predictions and labels for confusion matrix and metrics\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Identify misclassifications\n",
    "            for j in range(labels.size(0)):\n",
    "                if predicted[j] != labels[j]:\n",
    "                    misclassified_images.append(inputs[j].cpu())\n",
    "                    misclassified_labels.append(labels[j].cpu().numpy())\n",
    "                    misclassified_preds.append(predicted[j].cpu().numpy())\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    # Print the computed metrics\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Plot confusion matrix using seaborn with class names\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    return misclassified_images, misclassified_labels, misclassified_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 70865,
     "status": "ok",
     "timestamp": 1729608750417,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     },
     "user_tz": -480
    },
    "id": "BOQMEj3UoHcd",
    "outputId": "2da87674-3dbf-457d-a848-a9eb1c0525f1"
   },
   "outputs": [],
   "source": [
    "# Define class names based on the extracted labels\n",
    "class_names = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\"\n",
    "]\n",
    "\n",
    "# load the valid_subset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Path to the dataset in Google Drive\n",
    "\n",
    "# Test the model and display confusion matrix and metrics\n",
    "testloader = DataLoader(valid_subset, batch_size=128, shuffle=False, num_workers=4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load our trained model\n",
    "model_save_path = '/'.join(data_path.split('/')[:-3]) + \"/resnet_model_no_aug.pkl\"\n",
    "with open(model_save_path, 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "model.to(device)\n",
    "misclassified_images, misclassified_labels, misclassified_preds = test_model_with_confusion_matrix(model, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzB4tAq3sy6g"
   },
   "source": [
    "For the classes 0, 3, 4, 5 the model performed poorly. These classes will be augmented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--vyUZWat-IN"
   },
   "source": [
    "Lets print out some misclassified images out of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "executionInfo": {
     "elapsed": 2602,
     "status": "ok",
     "timestamp": 1729616698579,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     },
     "user_tz": -480
    },
    "id": "NFjJjH9xtYkg",
    "outputId": "662f28e6-d7b3-487d-9792-3649021718ac"
   },
   "outputs": [],
   "source": [
    "def display_misclassified_images(images, true_labels, predicted_labels, num_images=9):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    for i in range(min(num_images, len(images))):\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].permute(1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n",
    "        plt.title(f\"True: {class_names[true_labels[i]]}, Pred: {class_names[predicted_labels[i]]}\", fontsize=8)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display misclassified images\n",
    "display_misclassified_images(misclassified_images, misclassified_labels, misclassified_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUYMIcd4r0l_"
   },
   "source": [
    "Now evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 24444,
     "status": "ok",
     "timestamp": 1729608315041,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     },
     "user_tz": -480
    },
    "id": "EYUaB_PXrziV",
    "outputId": "89ffccb0-bb74-439b-c1a7-3673c715f6f0"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Path to the dataset in Google Drive\n",
    "data_path = '/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/cifar-10-python/cifar-10-batches-py/'\n",
    "test_batches = ['test_batch']\n",
    "\n",
    "# Default transform for other labels (ensure resizing is included)\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "# Define class names based on the extracted labels\n",
    "class_names = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\"\n",
    "]\n",
    "\n",
    "# Instantiate the test dataset\n",
    "test_dataset = CIFAR10Dataset(data_path, test_batches, transform=transform)\n",
    "\n",
    "# Test the model and display confusion matrix and metrics\n",
    "testloader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load our trained model\n",
    "model_save_path = '/'.join(data_path.split('/')[:-3]) + \"/resnet_model_no_aug.pkl\"\n",
    "with open(model_save_path, 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "misclassified_images, misclassified_labels, misclassified_preds = test_model_with_confusion_matrix(model, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2j2Cq5Rod2L"
   },
   "source": [
    "Now train the model again with augmentation on poorly classified labels!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5116841,
     "status": "ok",
     "timestamp": 1729624295740,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     },
     "user_tz": -480
    },
    "id": "ooh04KHGodCl",
    "outputId": "9e9990ad-8f16-4ca7-ddf8-67e554737752"
   },
   "outputs": [],
   "source": [
    "# lets train the model with augmentation\n",
    "model, train_losses, valid_losses, valid_accuracies = run_training(augment=True, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 1065,
     "status": "ok",
     "timestamp": 1729669376979,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     },
     "user_tz": -480
    },
    "id": "yMgcfQPQU_nB",
    "outputId": "84fae771-e8c1-456d-92b2-5ea29817139e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reading the CSV file\n",
    "df_new = pd.read_csv(\"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/training_results_augmentation.csv\")\n",
    "\n",
    "# Creating a figure and axis for the plot\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plotting Training and Validation Loss on the left axis\n",
    "ax1.plot(df_new[\"Epoch\"], df_new[\"Training Loss\"], label=\"Training Loss\", color=\"blue\")\n",
    "ax1.plot(df_new[\"Epoch\"], df_new[\"Validation Loss\"], label=\"Validation Loss\", color=\"orange\")\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend(loc=\"upper left\")\n",
    "\n",
    "# Creating a second y-axis for the accuracy\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(df_new[\"Epoch\"], df_new[\"Validation Accuracy\"], label=\"Validation Accuracy\", color=\"green\")\n",
    "ax2.set_ylabel('Validation Accuracy')\n",
    "ax2.legend(loc=\"upper right\")\n",
    "\n",
    "# Displaying the plot\n",
    "plt.title('Training/Validation Loss and Validation Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 78084,
     "status": "ok",
     "timestamp": 1729661041373,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     },
     "user_tz": -480
    },
    "id": "FE8ciMJnzoTw",
    "outputId": "e0b36889-d49a-46b5-cd20-cf978e1608aa"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load our trained model\n",
    "model_save_path = '/'.join(data_path.split('/')[:-3]) + \"/resnet_model_with_aug.pkl\"\n",
    "with open(model_save_path, 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "misclassified_images, misclassified_labels, misclassified_preds = test_model_with_confusion_matrix(model, validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "KCq5Jan9Sw9W",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "executionInfo": {
     "elapsed": 1912,
     "status": "ok",
     "timestamp": 1729661080795,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     },
     "user_tz": -480
    },
    "id": "FJtNglok1YVr",
    "outputId": "50936f55-0ade-4341-fa9c-715dc7841e8b"
   },
   "outputs": [],
   "source": [
    "def display_misclassified_images(images, true_labels, predicted_labels, num_images=9):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    for i in range(min(num_images, len(images))):\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].permute(1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n",
    "        plt.title(f\"True: {class_names[true_labels[i]]}, Pred: {class_names[predicted_labels[i]]}\", fontsize=8)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display misclassified images\n",
    "display_misclassified_images(misclassified_images, misclassified_labels, misclassified_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwhzhNTniBhc"
   },
   "source": [
    "Now evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 141165,
     "status": "ok",
     "timestamp": 1729624436904,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     },
     "user_tz": -480
    },
    "id": "8IP-m4BB5trZ",
    "outputId": "52337db0-4e73-4839-88ec-b1595b8267dc"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Path to the dataset in Google Drive\n",
    "data_path = '/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/cifar-10-python/cifar-10-batches-py/'\n",
    "test_batches = ['test_batch']\n",
    "\n",
    "# Default transform for other labels (ensure resizing is included)\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "# Define class names based on the extracted labels\n",
    "class_names = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\"\n",
    "]\n",
    "\n",
    "# Instantiate the test dataset\n",
    "test_dataset = CIFAR10Dataset(data_path, test_batches, transform=transform)\n",
    "\n",
    "# Test the model and display confusion matrix and metrics\n",
    "testloader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load our trained model\n",
    "model_save_path = '/'.join(data_path.split('/')[:-3]) + \"/resnet_model_with_aug.pkl\"\n",
    "with open(model_save_path, 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "misclassified_images, misclassified_labels, misclassified_preds = test_model_with_confusion_matrix(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQWb4gO2QtSE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKmeXS3EjD8T"
   },
   "source": [
    "Now train the model with the augmention on all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 633722,
     "status": "ok",
     "timestamp": 1729668446841,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     },
     "user_tz": -480
    },
    "id": "ak4hP-VSjHqo",
    "outputId": "00addc14-eea7-491c-b3b6-5612eb99dbc1"
   },
   "outputs": [],
   "source": [
    "# lets train the model with augmentation on all classes\n",
    "model, train_losses, valid_losses, valid_accuracies = run_training(augment=True, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 1426,
     "status": "ok",
     "timestamp": 1729669207235,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     },
     "user_tz": -480
    },
    "id": "v7iV97jjUQVu",
    "outputId": "4e3b11c7-2354-4f20-9d21-4667bf28a702"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Reading the CSV file\n",
    "df_newer = pd.read_csv(\"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/training_results_full_augmentation.csv\")\n",
    "\n",
    "# Creating a figure and axis for the plot\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plotting Training and Validation Loss on the left axis\n",
    "ax1.plot(df_newer[\"Epoch\"], df_newer[\"Training Loss\"], label=\"Training Loss\", color=\"blue\")\n",
    "ax1.plot(df_newer[\"Epoch\"], df_newer[\"Validation Loss\"], label=\"Validation Loss\", color=\"orange\")\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend(loc=\"upper left\")\n",
    "\n",
    "# Creating a second y-axis for the accuracy\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(df_newer[\"Epoch\"], df_newer[\"Validation Accuracy\"], label=\"Validation Accuracy\", color=\"green\")\n",
    "ax2.set_ylabel('Validation Accuracy')\n",
    "ax2.legend(loc=\"upper right\")\n",
    "\n",
    "# Displaying the plot\n",
    "plt.title('Training/Validation Loss and Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 78678,
     "status": "ok",
     "timestamp": 1729668580508,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     },
     "user_tz": -480
    },
    "id": "133f6CSZRiUw",
    "outputId": "8dd652de-738f-4065-804e-795c46421cf3"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load our trained model\n",
    "model_save_path = '/'.join(data_path.split('/')[:-3]) + \"/resnet_model_with_aug.pkl\"\n",
    "with open(model_save_path, 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "misclassified_images, misclassified_labels, misclassified_preds = test_model_with_confusion_matrix(model, validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "executionInfo": {
     "elapsed": 1289,
     "status": "ok",
     "timestamp": 1729668581765,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     },
     "user_tz": -480
    },
    "id": "Z5xI3beiRxn1",
    "outputId": "374ae106-9450-4d6d-aa4b-1c4bf91ba659"
   },
   "outputs": [],
   "source": [
    "def display_misclassified_images(images, true_labels, predicted_labels, num_images=9):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    for i in range(min(num_images, len(images))):\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].permute(1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n",
    "        plt.title(f\"True: {class_names[true_labels[i]]}, Pred: {class_names[predicted_labels[i]]}\", fontsize=8)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display misclassified images\n",
    "display_misclassified_images(misclassified_images, misclassified_labels, misclassified_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 28529,
     "status": "ok",
     "timestamp": 1729668620458,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     },
     "user_tz": -480
    },
    "id": "da0_RId5R03I",
    "outputId": "119c029f-9989-44f7-8f48-d28279a643d2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Path to the dataset in Google Drive\n",
    "data_path = '/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/cifar-10-python/cifar-10-batches-py/'\n",
    "test_batches = ['test_batch']\n",
    "\n",
    "# Default transform for other labels (ensure resizing is included)\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "# Define class names based on the extracted labels\n",
    "class_names = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\"\n",
    "]\n",
    "\n",
    "# Instantiate the test dataset\n",
    "test_dataset = CIFAR10Dataset(data_path, test_batches, transform=transform)\n",
    "\n",
    "# Test the model and display confusion matrix and metrics\n",
    "testloader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load our trained model\n",
    "model_save_path = '/'.join(data_path.split('/')[:-3]) + \"/resnet_model_with_aug.pkl\"\n",
    "with open(model_save_path, 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "misclassified_images, misclassified_labels, misclassified_preds = test_model_with_confusion_matrix(model, testloader)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMj6GYz3BoSb+RmDjW8zQSG",
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
