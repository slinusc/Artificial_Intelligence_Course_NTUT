{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "L4",
   "mount_file_id": "1t4AfRAtrcZCYkPgmvZGqtalrNYpumWww",
   "authorship_tag": "ABX9TyOUmbU9TV9w7qs5mPpP0FZ2"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Import necessary libraries"
   ],
   "metadata": {
    "id": "t7Zmj9HiMvYv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "id": "sU_iced07TO3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1729171898731,
     "user_tz": -480,
     "elapsed": 4108,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-10-18T07:03:26.088703Z",
     "start_time": "2024-10-18T07:03:26.078797Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data loader"
   ],
   "metadata": {
    "id": "zYh4CBkIM446"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Custom Dataset class for CIFAR-10 with lazy loading\n",
    "class CIFAR10Dataset(Dataset):\n",
    "    def __init__(self, data_path, batch_files, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.batch_files = batch_files\n",
    "        self.transform = transform\n",
    "        self.batch_data = None  # Only load the necessary batch when needed\n",
    "        self.batch_labels = None\n",
    "        self.batch_index = -1  # Track the currently loaded batch\n",
    "        self.index_map = []  # Maps dataset index to batch index and in-batch index\n",
    "        self._create_index_map()\n",
    "\n",
    "    def _create_index_map(self):\n",
    "        \"\"\"Create a map of global indices to batch indices.\"\"\"\n",
    "        start_idx = 0\n",
    "        for batch_num, batch_file in enumerate(self.batch_files):\n",
    "            with open(os.path.join(self.data_path, batch_file), 'rb') as f:\n",
    "                batch = pickle.load(f, encoding='bytes')\n",
    "                batch_size = len(batch[b'labels'])\n",
    "                self.index_map.extend([(batch_num, i) for i in range(batch_size)])\n",
    "            start_idx += batch_size\n",
    "\n",
    "    def _load_batch(self, batch_num):\n",
    "        \"\"\"Load a batch given its batch number.\"\"\"\n",
    "        batch_file = self.batch_files[batch_num]\n",
    "        with open(os.path.join(self.data_path, batch_file), 'rb') as f:\n",
    "            batch = pickle.load(f, encoding='bytes')\n",
    "            self.batch_data = batch[b'data'].reshape(-1, 3, 32, 32)\n",
    "            self.batch_labels = batch[b'labels']\n",
    "        self.batch_index = batch_num  # Update currently loaded batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Map global index to batch number and in-batch index\n",
    "        batch_num, in_batch_idx = self.index_map[idx]\n",
    "\n",
    "        # Load the batch if it's not already loaded\n",
    "        if batch_num != self.batch_index:\n",
    "            self._load_batch(batch_num)\n",
    "\n",
    "        # Fetch image and label from the loaded batch\n",
    "        image = self.batch_data[in_batch_idx]\n",
    "        label = self.batch_labels[in_batch_idx]\n",
    "\n",
    "        # Convert to the expected format (H x W x C)\n",
    "        image = image.transpose(1, 2, 0)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Transformations for CIFAR-10 (ResNet expects 224x224 images)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),  # ResNet-50 requires 224x224 input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n",
    "])"
   ],
   "metadata": {
    "id": "pJ0Qddj67W7h",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1729171898731,
     "user_tz": -480,
     "elapsed": 7,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-10-18T07:03:29.504148Z",
     "start_time": "2024-10-18T07:03:29.488450Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training with cross validation"
   ],
   "metadata": {
    "id": "tKmK77s0M8ME"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Custom function to train and evaluate a model with early stopping and validation accuracy\n",
    "def train_and_evaluate(trainloader, validloader, model, criterion, optimizer, num_epochs=10, patience=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # To store loss and accuracy for each epoch\n",
    "    train_losses, valid_losses = [], []\n",
    "    valid_accuracies = []  # To store validation accuracy\n",
    "    best_valid_loss = float('inf')\n",
    "    epochs_no_improve = 0  # Counter for early stopping\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        # Train on training data\n",
    "        progress_bar = tqdm(trainloader, desc=f'Epoch {epoch+1}/{num_epochs} (Train)')\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            total_train_samples += labels.size(0)\n",
    "\n",
    "        # Compute epoch training loss\n",
    "        epoch_train_loss = running_loss / total_train_samples\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        print(f'Training: Loss: {epoch_train_loss:.4f}')\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        running_valid_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_valid_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm(validloader, desc=f'Epoch {epoch+1}/{num_epochs} (Valid)')\n",
    "            for inputs, labels in progress_bar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                running_valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Get the predicted labels and calculate accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_valid_samples += labels.size(0)\n",
    "\n",
    "        # Compute epoch validation loss\n",
    "        epoch_valid_loss = running_valid_loss / total_valid_samples\n",
    "        valid_losses.append(epoch_valid_loss)\n",
    "        print(f'Validation: Loss: {epoch_valid_loss:.4f}')\n",
    "\n",
    "        # Compute validation accuracy\n",
    "        epoch_valid_acc = correct_predictions / total_valid_samples * 100\n",
    "        valid_accuracies.append(epoch_valid_acc)\n",
    "        print(f'Validation: Accuracy: {epoch_valid_acc:.2f}%')\n",
    "\n",
    "        # Check for early stopping\n",
    "        if epoch_valid_loss < best_valid_loss:\n",
    "            best_valid_loss = epoch_valid_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "    # Return the training and validation losses and validation accuracies\n",
    "    return train_losses, valid_losses, valid_accuracies\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "    ])\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "def k_fold_cross_validation(dataset, model_class, num_folds, num_epochs, batch_size, patience):\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "    # To store validation performance and learning curves across folds\n",
    "    all_train_losses, all_valid_losses, all_valid_accuracies = [], [], []\n",
    "\n",
    "    # K-fold Cross Validation\n",
    "    for fold, (train_idx, valid_idx) in enumerate(kfold.split(dataset)):\n",
    "        print(f'Fold {fold + 1}/{num_folds}')\n",
    "\n",
    "        # Subset the data for training and validation\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        valid_subset = Subset(dataset, valid_idx)\n",
    "\n",
    "        # Create DataLoaders for this fold - train is going to be augmented\n",
    "        trainloader = DataLoader(\n",
    "            [(train_transforms(image), label) for image, label in train_subset],\n",
    "            batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "        validloader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "        # Create a new instance of the model for each fold\n",
    "        model = model_class()\n",
    "\n",
    "        # Loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-2)\n",
    "\n",
    "        # Train and validate\n",
    "        train_losses, valid_losses, valid_accuracies = train_and_evaluate(\n",
    "            trainloader, validloader, model, criterion, optimizer, num_epochs=num_epochs, patience=patience\n",
    "        )\n",
    "\n",
    "        # Collect learning curves\n",
    "        all_train_losses.append(train_losses)\n",
    "        all_valid_losses.append(valid_losses)\n",
    "        all_valid_accuracies.append(valid_accuracies)\n",
    "\n",
    "    # Return learning curves for further plotting\n",
    "    return all_train_losses, all_valid_losses, all_valid_accuracies\n"
   ],
   "metadata": {
    "id": "1IJmRyvcDSrs",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1729171898731,
     "user_tz": -480,
     "elapsed": 7,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-10-18T07:08:18.434263520Z",
     "start_time": "2024-10-18T07:05:25.143776Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load data and run training"
   ],
   "metadata": {
    "id": "ZuNnGimTNKja"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Path to the dataset in Google Drive\n",
    "data_path = '/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/cifar-10-python/cifar-10-batches-py/'\n",
    "# Petr local path\n",
    "# data_path = '/home/sejvlpet/ai_data/cifar-10-batches-py/'\n",
    "\n",
    "# Training and test batch file names\n",
    "train_batches = [f'data_batch_{i}' for i in range(1, 6)]\n",
    "test_batches = ['test_batch']\n",
    "\n",
    "# Define the necessary transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),  # ResNet-50 requires 224x224 input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n",
    "])\n",
    "\n",
    "# Create Dataset instance for the full training dataset\n",
    "train_dataset = CIFAR10Dataset(data_path, train_batches, transform=transform)\n",
    "\n",
    "# Perform 5-fold cross-validation on the dataset with ResNet-50 and save the losses\n",
    "train_losses, valid_losses = k_fold_cross_validation(train_dataset, lambda: resnet50(weights=ResNet50_Weights.DEFAULT), num_folds=5, num_epochs=5, batch_size=16, patience=2)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UbxCms8wKXpa",
    "outputId": "872cc1ac-fd00-49b6-cb85-071f50196632",
    "ExecuteTime": {
     "end_time": "2024-10-18T07:08:18.427955Z",
     "start_time": "2024-10-18T07:05:27.462891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /home/sejvlpet/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|██████████| 97.8M/97.8M [00:19<00:00, 5.30MB/s]\n",
      "Epoch 1/5 (Train):   0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Train):   4%|▍         | 1/25 [00:08<03:34,  8.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Train):   8%|▊         | 2/25 [00:13<02:26,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Train):  12%|█▏        | 3/25 [00:18<02:03,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Train):  16%|█▌        | 4/25 [00:23<01:56,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Train):  20%|██        | 5/25 [00:28<01:47,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Train):  24%|██▍       | 6/25 [00:34<01:46,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Train):  28%|██▊       | 7/25 [00:39<01:35,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Train):  32%|███▏      | 8/25 [00:46<01:36,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Train):  36%|███▌      | 9/25 [00:53<01:38,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Train):  40%|████      | 10/25 [00:57<01:23,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Train):  44%|████▍     | 11/25 [01:01<01:13,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Train):  48%|████▊     | 12/25 [01:06<01:06,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Train):  52%|█████▏    | 13/25 [01:11<00:58,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Train):  56%|█████▌    | 14/25 [01:16<00:53,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Train):  60%|██████    | 15/25 [01:21<00:50,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Train):  64%|██████▍   | 16/25 [01:25<00:43,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Train):  68%|██████▊   | 17/25 [01:31<00:39,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Train):  72%|███████▏  | 18/25 [01:35<00:33,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Train):  76%|███████▌  | 19/25 [01:39<00:28,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Train):  80%|████████  | 20/25 [01:44<00:22,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Train):  84%|████████▍ | 21/25 [01:48<00:17,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Train):  88%|████████▊ | 22/25 [01:52<00:13,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Train):  92%|█████████▏| 23/25 [01:56<00:08,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Train):  96%|█████████▌| 24/25 [02:00<00:04,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Train): 100%|██████████| 25/25 [02:06<00:00,  5.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Loss: 3.2530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Valid): 100%|██████████| 7/7 [00:10<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss: 2.5016\n",
      "Validation: Accuracy: 24.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 (Train):   0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 (Train):   4%|▍         | 1/25 [00:05<02:19,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 (Train):   8%|▊         | 2/25 [00:10<01:58,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train loader and not failing lol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 (Train):   8%|▊         | 2/25 [00:11<02:07,  5.56s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 22\u001B[0m\n\u001B[1;32m     19\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m CIFAR10Dataset(data_path, train_batches, transform\u001B[38;5;241m=\u001B[39mtransform)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Perform 5-fold cross-validation on the dataset with ResNet-50 and save the losses\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m train_losses, valid_losses \u001B[38;5;241m=\u001B[39m \u001B[43mk_fold_cross_validation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresnet50\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mResNet50_Weights\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDEFAULT\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_folds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpatience\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[8], line 121\u001B[0m, in \u001B[0;36mk_fold_cross_validation\u001B[0;34m(dataset, model_class, num_folds, num_epochs, batch_size, patience)\u001B[0m\n\u001B[1;32m    118\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdamW(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m, weight_decay\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-2\u001B[39m)\n\u001B[1;32m    120\u001B[0m \u001B[38;5;66;03m# Train and validate\u001B[39;00m\n\u001B[0;32m--> 121\u001B[0m train_losses, valid_losses, valid_accuracies \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_and_evaluate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    122\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrainloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpatience\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpatience\u001B[49m\n\u001B[1;32m    123\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    125\u001B[0m \u001B[38;5;66;03m# Collect learning curves\u001B[39;00m\n\u001B[1;32m    126\u001B[0m all_train_losses\u001B[38;5;241m.\u001B[39mappend(train_losses)\n",
      "Cell \u001B[0;32mIn[8], line 26\u001B[0m, in \u001B[0;36mtrain_and_evaluate\u001B[0;34m(trainloader, validloader, model, criterion, optimizer, num_epochs, patience)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124musing train loader and not failing lol\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     25\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 26\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[1;32m     28\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001B[0m, in \u001B[0;36mResNet.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/resnet.py:273\u001B[0m, in \u001B[0;36mResNet._forward_impl\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    270\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(x)\n\u001B[1;32m    271\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmaxpool(x)\n\u001B[0;32m--> 273\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    274\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer2(x)\n\u001B[1;32m    275\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer3(x)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/resnet.py:146\u001B[0m, in \u001B[0;36mBottleneck.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m    144\u001B[0m     identity \u001B[38;5;241m=\u001B[39m x\n\u001B[0;32m--> 146\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    147\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn1(out)\n\u001B[1;32m    148\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(out)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluation on test set"
   ],
   "metadata": {
    "id": "pdzUrP2GNP6q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def test_model_with_confusion_matrix(model, testloader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(enumerate(testloader), total=len(testloader), desc='Testing')\n",
    "        for i, (inputs, labels) in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Append predictions and labels for confusion matrix and metrics\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    # Print the computed metrics\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Plot confusion matrix using seaborn\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[i for i in range(10)], yticklabels=[i for i in range(10)])\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Test the model and display confusion matrix and metrics\n",
    "test_model_with_confusion_matrix(model, testloader)"
   ],
   "metadata": {
    "id": "7iC9hDLuDeOC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plotting learning curves"
   ],
   "metadata": {
    "id": "hbucirhENW8h"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_learning_curves(train_losses, valid_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(valid_losses, label='Validation Loss')\n",
    "    plt.plot(valid_accuracies, label='Validation Accuracy')\n",
    "    plt.title('Learning Curves')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss' if 'Loss' in train_losses[0] else 'Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(train_losses, valid_losses)"
   ],
   "metadata": {
    "id": "j6TIN-46KLki"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
