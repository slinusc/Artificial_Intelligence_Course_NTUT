{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "L4",
   "mount_file_id": "1t4AfRAtrcZCYkPgmvZGqtalrNYpumWww",
   "authorship_tag": "ABX9TyOUmbU9TV9w7qs5mPpP0FZ2"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Import necessary libraries"
   ],
   "metadata": {
    "id": "t7Zmj9HiMvYv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "id": "sU_iced07TO3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1729171898731,
     "user_tz": -480,
     "elapsed": 4108,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-10-18T07:03:26.088703Z",
     "start_time": "2024-10-18T07:03:26.078797Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data loader"
   ],
   "metadata": {
    "id": "zYh4CBkIM446"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Custom Dataset class for CIFAR-10 with lazy loading\n",
    "class CIFAR10Dataset(Dataset):\n",
    "    def __init__(self, data_path, batch_files, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.batch_files = batch_files\n",
    "        self.transform = transform\n",
    "        self.batch_data = None  # Only load the necessary batch when needed\n",
    "        self.batch_labels = None\n",
    "        self.batch_index = -1  # Track the currently loaded batch\n",
    "        self.index_map = []  # Maps dataset index to batch index and in-batch index\n",
    "        self._create_index_map()\n",
    "\n",
    "    def _create_index_map(self):\n",
    "        \"\"\"Create a map of global indices to batch indices.\"\"\"\n",
    "        start_idx = 0\n",
    "        for batch_num, batch_file in enumerate(self.batch_files):\n",
    "            with open(os.path.join(self.data_path, batch_file), 'rb') as f:\n",
    "                batch = pickle.load(f, encoding='bytes')\n",
    "                batch_size = len(batch[b'labels'])\n",
    "                self.index_map.extend([(batch_num, i) for i in range(batch_size)])\n",
    "            start_idx += batch_size\n",
    "\n",
    "    def _load_batch(self, batch_num):\n",
    "        \"\"\"Load a batch given its batch number.\"\"\"\n",
    "        batch_file = self.batch_files[batch_num]\n",
    "        with open(os.path.join(self.data_path, batch_file), 'rb') as f:\n",
    "            batch = pickle.load(f, encoding='bytes')\n",
    "            self.batch_data = batch[b'data'].reshape(-1, 3, 32, 32)\n",
    "            self.batch_labels = batch[b'labels']\n",
    "        self.batch_index = batch_num  # Update currently loaded batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Map global index to batch number and in-batch index\n",
    "        batch_num, in_batch_idx = self.index_map[idx]\n",
    "\n",
    "        # Load the batch if it's not already loaded\n",
    "        if batch_num != self.batch_index:\n",
    "            self._load_batch(batch_num)\n",
    "\n",
    "        # Fetch image and label from the loaded batch\n",
    "        image = self.batch_data[in_batch_idx]\n",
    "        label = self.batch_labels[in_batch_idx]\n",
    "\n",
    "        # Convert to the expected format (H x W x C)\n",
    "        image = image.transpose(1, 2, 0)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Transformations for CIFAR-10 (ResNet expects 224x224 images)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),  # ResNet-50 requires 224x224 input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n",
    "])"
   ],
   "metadata": {
    "id": "pJ0Qddj67W7h",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1729171898731,
     "user_tz": -480,
     "elapsed": 7,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-10-18T07:03:29.504148Z",
     "start_time": "2024-10-18T07:03:29.488450Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training with cross validation"
   ],
   "metadata": {
    "id": "tKmK77s0M8ME"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Custom function to train and evaluate a model with early stopping and validation accuracy\n",
    "def train_and_evaluate(trainloader, validloader, model, criterion, optimizer, num_epochs=10, patience=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # To store loss and accuracy for each epoch\n",
    "    train_losses, valid_losses = [], []\n",
    "    valid_accuracies = []  # To store validation accuracy\n",
    "    best_valid_loss = float('inf')\n",
    "    epochs_no_improve = 0  # Counter for early stopping\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        # Train on training data\n",
    "        progress_bar = tqdm(trainloader, desc=f'Epoch {epoch+1}/{num_epochs} (Train)')\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            total_train_samples += labels.size(0)\n",
    "\n",
    "        # Compute epoch training loss\n",
    "        epoch_train_loss = running_loss / total_train_samples\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        print(f'Training: Loss: {epoch_train_loss:.4f}')\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        running_valid_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_valid_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm(validloader, desc=f'Epoch {epoch+1}/{num_epochs} (Valid)')\n",
    "            for inputs, labels in progress_bar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                running_valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Get the predicted labels and calculate accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_valid_samples += labels.size(0)\n",
    "\n",
    "        # Compute epoch validation loss\n",
    "        epoch_valid_loss = running_valid_loss / total_valid_samples\n",
    "        valid_losses.append(epoch_valid_loss)\n",
    "        print(f'Validation: Loss: {epoch_valid_loss:.4f}')\n",
    "\n",
    "        # Compute validation accuracy\n",
    "        epoch_valid_acc = correct_predictions / total_valid_samples * 100\n",
    "        valid_accuracies.append(epoch_valid_acc)\n",
    "        print(f'Validation: Accuracy: {epoch_valid_acc:.2f}%')\n",
    "\n",
    "        # Check for early stopping\n",
    "        if epoch_valid_loss < best_valid_loss:\n",
    "            best_valid_loss = epoch_valid_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "    # Return the training and validation losses and validation accuracies\n",
    "    return train_losses, valid_losses, valid_accuracies\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "    ])\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "def k_fold_cross_validation(dataset, model_class, num_folds, num_epochs, batch_size, patience):\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "    # To store validation performance and learning curves across folds\n",
    "    all_train_losses, all_valid_losses, all_valid_accuracies, models = [], [], [], []\n",
    "\n",
    "    # K-fold Cross Validation\n",
    "    for fold, (train_idx, valid_idx) in enumerate(kfold.split(dataset)):\n",
    "        print(f'Fold {fold + 1}/{num_folds}')\n",
    "\n",
    "        # Subset the data for training and validation\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        valid_subset = Subset(dataset, valid_idx)\n",
    "\n",
    "        # Create DataLoaders for this fold - train is going to be augmented\n",
    "        trainloader = DataLoader(\n",
    "            [(train_transforms(image), label) for image, label in train_subset],\n",
    "            batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "        validloader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "        # Create a new instance of the model for each fold\n",
    "        model = model_class()\n",
    "\n",
    "        # Loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-2)\n",
    "\n",
    "        # Train and validate\n",
    "        train_losses, valid_losses, valid_accuracies = train_and_evaluate(\n",
    "            trainloader, validloader, model, criterion, optimizer, num_epochs=num_epochs, patience=patience\n",
    "        )\n",
    "\n",
    "        # Collect learning curves\n",
    "        all_train_losses.append(train_losses)\n",
    "        all_valid_losses.append(valid_losses)\n",
    "        all_valid_accuracies.append(valid_accuracies)\n",
    "        models.append(model)\n",
    "\n",
    "    # Return learning curves for further plotting\n",
    "    return all_train_losses, all_valid_losses, all_valid_accuracies, models\n"
   ],
   "metadata": {
    "id": "1IJmRyvcDSrs",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1729171898731,
     "user_tz": -480,
     "elapsed": 7,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-10-18T07:08:18.434263520Z",
     "start_time": "2024-10-18T07:05:25.143776Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load data and run training"
   ],
   "metadata": {
    "id": "ZuNnGimTNKja"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Path to the dataset in Google Drive\n",
    "data_path = '/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/cifar-10-python/cifar-10-batches-py/'\n",
    "# Petr local path\n",
    "# data_path = '/home/sejvlpet/ai_data/cifar-10-batches-py/'\n",
    "\n",
    "# Training and test batch file names\n",
    "train_batches = [f'data_batch_{i}' for i in range(1, 6)]\n",
    "test_batches = ['test_batch']\n",
    "\n",
    "# Define the necessary transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),  # ResNet-50 requires 224x224 input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n",
    "])\n",
    "\n",
    "# Create Dataset instance for the full training dataset\n",
    "train_dataset = CIFAR10Dataset(data_path, train_batches, transform=transform)\n",
    "\n",
    "# Perform 5-fold cross-validation on the dataset with ResNet-50 and save the losses\n",
    "train_losses, valid_losses, _, models = k_fold_cross_validation(train_dataset, lambda: resnet50(weights=ResNet50_Weights.DEFAULT), num_folds=5, num_epochs=5, batch_size=16, patience=2)\n",
    "\n",
    "# Save the model\n",
    "model_save_path = data_path.split('/')[:-2].join('/') + \"models.pkl\"\n",
    "with open(model_save_path, 'wb') as f:\n",
    "    pickle.dump(models, f)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UbxCms8wKXpa",
    "outputId": "872cc1ac-fd00-49b6-cb85-071f50196632",
    "ExecuteTime": {
     "end_time": "2024-10-18T07:27:09.224778Z",
     "start_time": "2024-10-18T07:27:09.157380Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/cifar-10-python/cifar-10-batches-py/data_batch_1'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 19\u001B[0m\n\u001B[1;32m     11\u001B[0m transform \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mCompose([\n\u001B[1;32m     12\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mToPILImage(),\n\u001B[1;32m     13\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mResize((\u001B[38;5;241m224\u001B[39m, \u001B[38;5;241m224\u001B[39m)),  \u001B[38;5;66;03m# ResNet-50 requires 224x224 input size\u001B[39;00m\n\u001B[1;32m     14\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mToTensor(),\n\u001B[1;32m     15\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mNormalize((\u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0.5\u001B[39m), (\u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0.5\u001B[39m))  \u001B[38;5;66;03m# Normalize the images\u001B[39;00m\n\u001B[1;32m     16\u001B[0m ])\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Create Dataset instance for the full training dataset\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mCIFAR10Dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_batches\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Perform 5-fold cross-validation on the dataset with ResNet-50 and save the losses\u001B[39;00m\n\u001B[1;32m     22\u001B[0m train_losses, valid_losses, _, models \u001B[38;5;241m=\u001B[39m k_fold_cross_validation(train_dataset, \u001B[38;5;28;01mlambda\u001B[39;00m: resnet50(weights\u001B[38;5;241m=\u001B[39mResNet50_Weights\u001B[38;5;241m.\u001B[39mDEFAULT), num_folds\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, num_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m, patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n",
      "Cell \u001B[0;32mIn[3], line 11\u001B[0m, in \u001B[0;36mCIFAR10Dataset.__init__\u001B[0;34m(self, data_path, batch_files, transform)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m  \u001B[38;5;66;03m# Track the currently loaded batch\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex_map \u001B[38;5;241m=\u001B[39m []  \u001B[38;5;66;03m# Maps dataset index to batch index and in-batch index\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_index_map\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[3], line 17\u001B[0m, in \u001B[0;36mCIFAR10Dataset._create_index_map\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     15\u001B[0m start_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_num, batch_file \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_files):\n\u001B[0;32m---> 17\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_file\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m     18\u001B[0m         batch \u001B[38;5;241m=\u001B[39m pickle\u001B[38;5;241m.\u001B[39mload(f, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbytes\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     19\u001B[0m         batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch[\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001B[0m, in \u001B[0;36m_modified_open\u001B[0;34m(file, *args, **kwargs)\u001B[0m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[1;32m    318\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    319\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    320\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    321\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    322\u001B[0m     )\n\u001B[0;32m--> 324\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/cifar-10-python/cifar-10-batches-py/data_batch_1'"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluation on test set"
   ],
   "metadata": {
    "id": "pdzUrP2GNP6q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def test_model_with_confusion_matrix(model, testloader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(enumerate(testloader), total=len(testloader), desc='Testing')\n",
    "        for i, (inputs, labels) in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Append predictions and labels for confusion matrix and metrics\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    # Print the computed metrics\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Plot confusion matrix using seaborn\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[i for i in range(10)], yticklabels=[i for i in range(10)])\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Test the model and display confusion matrix and metrics\n",
    "test_model_with_confusion_matrix(model, testloader)"
   ],
   "metadata": {
    "id": "7iC9hDLuDeOC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plotting learning curves"
   ],
   "metadata": {
    "id": "hbucirhENW8h"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_learning_curves(train_losses, valid_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(valid_losses, label='Validation Loss')\n",
    "    plt.plot(valid_accuracies, label='Validation Accuracy')\n",
    "    plt.title('Learning Curves')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss' if 'Loss' in train_losses[0] else 'Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(train_losses, valid_losses)"
   ],
   "metadata": {
    "id": "j6TIN-46KLki"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
