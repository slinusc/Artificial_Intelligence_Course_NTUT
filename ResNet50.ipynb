{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","mount_file_id":"1t4AfRAtrcZCYkPgmvZGqtalrNYpumWww","authorship_tag":"ABX9TyP9ZVLbb3bDp13Ks1w+8ynB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Import nessesary libraries"],"metadata":{"id":"t7Zmj9HiMvYv"}},{"cell_type":"code","source":["import os\n","import pickle\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader, Subset\n","from torchvision import transforms\n","from torchvision.models import resnet50, ResNet50_Weights\n","\n","import torch.optim as optim\n","import torch.nn as nn\n","from sklearn.model_selection import KFold\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n","import seaborn as sns"],"metadata":{"id":"sU_iced07TO3","executionInfo":{"status":"ok","timestamp":1729163496367,"user_tz":-480,"elapsed":4009,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["Data loader"],"metadata":{"id":"zYh4CBkIM446"}},{"cell_type":"code","source":["# Custom Dataset class for CIFAR-10 with lazy loading\n","class CIFAR10Dataset(Dataset):\n","    def __init__(self, data_path, batch_files, transform=None):\n","        self.data_path = data_path\n","        self.batch_files = batch_files\n","        self.transform = transform\n","        self.batch_data = None  # Only load the necessary batch when needed\n","        self.batch_labels = None\n","        self.batch_index = -1  # Track the currently loaded batch\n","        self.index_map = []  # Maps dataset index to batch index and in-batch index\n","        self._create_index_map()\n","\n","    def _create_index_map(self):\n","        \"\"\"Create a map of global indices to batch indices.\"\"\"\n","        start_idx = 0\n","        for batch_num, batch_file in enumerate(self.batch_files):\n","            with open(os.path.join(self.data_path, batch_file), 'rb') as f:\n","                batch = pickle.load(f, encoding='bytes')\n","                batch_size = len(batch[b'labels'])\n","                self.index_map.extend([(batch_num, i) for i in range(batch_size)])\n","            start_idx += batch_size\n","\n","    def _load_batch(self, batch_num):\n","        \"\"\"Load a batch given its batch number.\"\"\"\n","        batch_file = self.batch_files[batch_num]\n","        with open(os.path.join(self.data_path, batch_file), 'rb') as f:\n","            batch = pickle.load(f, encoding='bytes')\n","            self.batch_data = batch[b'data'].reshape(-1, 3, 32, 32)\n","            self.batch_labels = batch[b'labels']\n","        self.batch_index = batch_num  # Update currently loaded batch\n","\n","    def __len__(self):\n","        return len(self.index_map)\n","\n","    def __getitem__(self, idx):\n","        # Map global index to batch number and in-batch index\n","        batch_num, in_batch_idx = self.index_map[idx]\n","\n","        # Load the batch if it's not already loaded\n","        if batch_num != self.batch_index:\n","            self._load_batch(batch_num)\n","\n","        # Fetch image and label from the loaded batch\n","        image = self.batch_data[in_batch_idx]\n","        label = self.batch_labels[in_batch_idx]\n","\n","        # Convert to the expected format (H x W x C)\n","        image = image.transpose(1, 2, 0)\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label\n","\n","\n","# Transformations for CIFAR-10 (ResNet expects 224x224 images)\n","transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((224, 224)),  # ResNet-50 requires 224x224 input size\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n","])"],"metadata":{"id":"pJ0Qddj67W7h","executionInfo":{"status":"ok","timestamp":1729163496367,"user_tz":-480,"elapsed":2,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Training with cross validation"],"metadata":{"id":"tKmK77s0M8ME"}},{"cell_type":"code","source":["# Custom function to train and evaluate a model with early stopping\n","def train_and_evaluate(trainloader, validloader, model, criterion, optimizer, num_epochs=10, patience=5):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = model.to(device)\n","\n","    # To store loss for each epoch\n","    train_losses, valid_losses = [], []\n","    best_valid_loss = float('inf')\n","    epochs_no_improve = 0  # Counter for early stopping\n","\n","    # Training loop\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        total_train_samples = 0\n","\n","        # Train on training data\n","        progress_bar = tqdm(trainloader, desc=f'Epoch {epoch+1}/{num_epochs} (Train)')\n","        for inputs, labels in progress_bar:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * inputs.size(0)\n","            total_train_samples += labels.size(0)\n","\n","        # Compute epoch training loss\n","        epoch_train_loss = running_loss / total_train_samples\n","        train_losses.append(epoch_train_loss)\n","        print(f'Training: Loss: {epoch_train_loss:.4f}')\n","\n","        # Validation loop\n","        model.eval()\n","        running_valid_loss = 0.0\n","        total_valid_samples = 0\n","\n","        with torch.no_grad():\n","            progress_bar = tqdm(validloader, desc=f'Epoch {epoch+1}/{num_epochs} (Valid)')\n","            for inputs, labels in progress_bar:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","\n","                running_valid_loss += loss.item() * inputs.size(0)\n","                total_valid_samples += labels.size(0)\n","\n","        # Compute epoch validation loss\n","        epoch_valid_loss = running_valid_loss / total_valid_samples\n","        valid_losses.append(epoch_valid_loss)\n","        print(f'Validation: Loss: {epoch_valid_loss:.4f}')\n","\n","        # Check for early stopping\n","        if epoch_valid_loss < best_valid_loss:\n","            best_valid_loss = epoch_valid_loss\n","            epochs_no_improve = 0\n","        else:\n","            epochs_no_improve += 1\n","\n","        if epochs_no_improve >= patience:\n","            print(f'Early stopping at epoch {epoch+1}')\n","            break\n","\n","    # Return the training and validation losses\n","    return train_losses, valid_losses\n","\n","\n","# K-Fold Cross Validation\n","def k_fold_cross_validation(dataset, model_class, num_folds=5, num_epochs=10, batch_size=32, patience=5):\n","    kfold = KFold(n_splits=num_folds, shuffle=True)\n","\n","    # To store validation performance and learning curves across folds\n","    all_train_losses, all_valid_losses = [], []\n","\n","    # K-fold Cross Validation\n","    for fold, (train_idx, valid_idx) in enumerate(kfold.split(dataset)):\n","        print(f'Fold {fold + 1}/{num_folds}')\n","\n","        # Subset the data for training and validation\n","        train_subset = Subset(dataset, train_idx)\n","        valid_subset = Subset(dataset, valid_idx)\n","\n","        # Create DataLoaders for this fold\n","        trainloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n","        validloader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False, num_workers=4)\n","\n","        # Create a new instance of the model for each fold\n","        model = model_class()\n","\n","        # Loss function and optimizer\n","        criterion = nn.CrossEntropyLoss()\n","        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-2)\n","\n","        # Train and validate\n","        train_losses, valid_losses = train_and_evaluate(trainloader, validloader, model, criterion, optimizer, num_epochs=num_epochs, patience=patience)\n","\n","        # Collect learning curves\n","        all_train_losses.append(train_losses)\n","        all_valid_losses.append(valid_losses)\n","\n","    # Return learning curves for further plotting\n","    return all_train_losses, all_valid_losses"],"metadata":{"id":"1IJmRyvcDSrs","executionInfo":{"status":"ok","timestamp":1729163496367,"user_tz":-480,"elapsed":2,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Load data and run training"],"metadata":{"id":"ZuNnGimTNKja"}},{"cell_type":"code","source":["# Path to the dataset in your Google Drive\n","data_path = '/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/cifar-10-python/cifar-10-batches-py/'\n","\n","# Training and test batch file names\n","train_batches = [f'data_batch_{i}' for i in range(1, 6)]\n","test_batches = ['test_batch']\n","\n","# Define the necessary transforms\n","transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((224, 224)),  # ResNet-50 requires 224x224 input size\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n","])\n","\n","# Create Dataset instance for the full training dataset\n","train_dataset = CIFAR10Dataset(data_path, train_batches, transform=transform)\n","\n","# Perform 5-fold cross-validation on the dataset with ResNet-50 and save the losses\n","# Here you only define the batch size once inside the k_fold_cross_validation function\n","train_losses, valid_losses = k_fold_cross_validation(train_dataset, lambda: resnet50(weights=ResNet50_Weights.DEFAULT), num_folds=5, num_epochs=5, batch_size=128, patience=2)\n","\n","# Plot the learning curves for the first fold as an example\n","plot_learning_curves(train_losses[0], valid_losses[0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UbxCms8wKXpa","outputId":"8870af43-01e9-498b-e094-10032ce41f74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fold 1/5\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/10 (Train):  50%|█████     | 157/313 [05:37<05:35,  2.15s/it]"]}]},{"cell_type":"markdown","source":["Evaluation on test set"],"metadata":{"id":"pdzUrP2GNP6q"}},{"cell_type":"code","source":["def test_model_with_confusion_matrix(model, testloader):\n","    model.eval()  # Set model to evaluation mode\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        progress_bar = tqdm(enumerate(testloader), total=len(testloader), desc='Testing')\n","        for i, (inputs, labels) in progress_bar:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs, 1)\n","\n","            # Append predictions and labels for confusion matrix and metrics\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    # Generate the confusion matrix\n","    cm = confusion_matrix(all_labels, all_preds)\n","    print(f\"Confusion Matrix:\\n{cm}\")\n","\n","    # Calculate metrics\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    precision = precision_score(all_labels, all_preds, average='weighted')\n","    recall = recall_score(all_labels, all_preds, average='weighted')\n","    f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","    # Print the computed metrics\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1 Score: {f1:.4f}\")\n","\n","    # Plot confusion matrix using seaborn\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[i for i in range(10)], yticklabels=[i for i in range(10)])\n","    plt.ylabel('Actual')\n","    plt.xlabel('Predicted')\n","    plt.title('Confusion Matrix')\n","    plt.show()\n","\n","# Test the model and display confusion matrix and metrics\n","test_model_with_confusion_matrix(model, testloader)"],"metadata":{"id":"7iC9hDLuDeOC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plotting learning curves"],"metadata":{"id":"hbucirhENW8h"}},{"cell_type":"code","source":["def plot_learning_curves(train_losses, valid_losses):\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(train_losses, label='Training Loss')\n","    plt.plot(valid_losses, label='Validation Loss')\n","    plt.title('Learning Curves')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.show()\n","\n","\n","plot_learning_curves(train_losses, valid_losses)"],"metadata":{"id":"j6TIN-46KLki"},"execution_count":null,"outputs":[]}]}