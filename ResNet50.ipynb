{"cells":[{"cell_type":"markdown","metadata":{"id":"t7Zmj9HiMvYv"},"source":["Import necessary libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4108,"status":"ok","timestamp":1729171898731,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"},"user_tz":-480},"id":"sU_iced07TO3"},"outputs":[],"source":["import os\n","import pickle\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader, Subset\n","from torchvision import transforms\n","from torchvision.models import resnet50, ResNet50_Weights\n","\n","import torch.optim as optim\n","import torch.nn as nn\n","from sklearn.model_selection import KFold\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n","import seaborn as sns"]},{"cell_type":"markdown","metadata":{"id":"zYh4CBkIM446"},"source":["Data loader"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1729171898731,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"},"user_tz":-480},"id":"pJ0Qddj67W7h"},"outputs":[],"source":["# Custom Dataset class for CIFAR-10 with lazy loading\n","class CIFAR10Dataset(Dataset):\n","    def __init__(self, data_path, batch_files, transform=None):\n","        self.data_path = data_path\n","        self.batch_files = batch_files\n","        self.transform = transform\n","        self.batch_data = None  # Only load the necessary batch when needed\n","        self.batch_labels = None\n","        self.batch_index = -1  # Track the currently loaded batch\n","        self.index_map = []  # Maps dataset index to batch index and in-batch index\n","        self._create_index_map()\n","\n","    def _create_index_map(self):\n","        \"\"\"Create a map of global indices to batch indices.\"\"\"\n","        start_idx = 0\n","        for batch_num, batch_file in enumerate(self.batch_files):\n","            with open(os.path.join(self.data_path, batch_file), 'rb') as f:\n","                batch = pickle.load(f, encoding='bytes')\n","                batch_size = len(batch[b'labels'])\n","                self.index_map.extend([(batch_num, i) for i in range(batch_size)])\n","            start_idx += batch_size\n","\n","    def _load_batch(self, batch_num):\n","        \"\"\"Load a batch given its batch number.\"\"\"\n","        batch_file = self.batch_files[batch_num]\n","        with open(os.path.join(self.data_path, batch_file), 'rb') as f:\n","            batch = pickle.load(f, encoding='bytes')\n","            self.batch_data = batch[b'data'].reshape(-1, 3, 32, 32)\n","            self.batch_labels = batch[b'labels']\n","        self.batch_index = batch_num  # Update currently loaded batch\n","\n","    def __len__(self):\n","        return len(self.index_map)\n","\n","    def __getitem__(self, idx):\n","        # Map global index to batch number and in-batch index\n","        batch_num, in_batch_idx = self.index_map[idx]\n","\n","        # Load the batch if it's not already loaded\n","        if batch_num != self.batch_index:\n","            self._load_batch(batch_num)\n","\n","        # Fetch image and label from the loaded batch\n","        image = self.batch_data[in_batch_idx]\n","        label = self.batch_labels[in_batch_idx]\n","\n","        # Convert to the expected format (H x W x C)\n","        image = image.transpose(1, 2, 0)\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label\n","\n","\n","# Transformations for CIFAR-10 (ResNet expects 224x224 images)\n","transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((224, 224)),  # ResNet-50 requires 224x224 input size\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n","])"]},{"cell_type":"markdown","metadata":{"id":"tKmK77s0M8ME"},"source":["Training with cross validation"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1729171898731,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"},"user_tz":-480},"id":"1IJmRyvcDSrs"},"outputs":[],"source":["# Custom function to train and evaluate a model with early stopping and validation accuracy\n","def train_and_evaluate(trainloader, validloader, model, criterion, optimizer, num_epochs=10, patience=5):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = model.to(device)\n","\n","    # To store loss and accuracy for each epoch\n","    train_losses, valid_losses = [], []\n","    valid_accuracies = []  # To store validation accuracy\n","    best_valid_loss = float('inf')\n","    epochs_no_improve = 0  # Counter for early stopping\n","\n","    # Training loop\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        total_train_samples = 0\n","\n","        # Train on training data\n","        progress_bar = tqdm(trainloader, desc=f'Epoch {epoch+1}/{num_epochs} (Train)')\n","        for inputs, labels in progress_bar:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * inputs.size(0)\n","            total_train_samples += labels.size(0)\n","\n","        # Compute epoch training loss\n","        epoch_train_loss = running_loss / total_train_samples\n","        train_losses.append(epoch_train_loss)\n","        print(f'Training: Loss: {epoch_train_loss:.4f}')\n","\n","        # Validation loop\n","        model.eval()\n","        running_valid_loss = 0.0\n","        correct_predictions = 0\n","        total_valid_samples = 0\n","\n","        with torch.no_grad():\n","            progress_bar = tqdm(validloader, desc=f'Epoch {epoch+1}/{num_epochs} (Valid)')\n","            for inputs, labels in progress_bar:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","\n","                running_valid_loss += loss.item() * inputs.size(0)\n","\n","                # Get the predicted labels and calculate accuracy\n","                _, predicted = torch.max(outputs, 1)\n","                correct_predictions += (predicted == labels).sum().item()\n","                total_valid_samples += labels.size(0)\n","\n","        # Compute epoch validation loss\n","        epoch_valid_loss = running_valid_loss / total_valid_samples\n","        valid_losses.append(epoch_valid_loss)\n","        print(f'Validation: Loss: {epoch_valid_loss:.4f}')\n","\n","        # Compute validation accuracy\n","        epoch_valid_acc = correct_predictions / total_valid_samples * 100\n","        valid_accuracies.append(epoch_valid_acc)\n","        print(f'Validation: Accuracy: {epoch_valid_acc:.2f}%')\n","\n","        # Check for early stopping\n","        if epoch_valid_loss \u003c best_valid_loss:\n","            best_valid_loss = epoch_valid_loss\n","            epochs_no_improve = 0\n","        else:\n","            epochs_no_improve += 1\n","\n","        if epochs_no_improve \u003e= patience:\n","            print(f'Early stopping at epoch {epoch+1}')\n","            break\n","\n","    # Return the training and validation losses and validation accuracies\n","    return train_losses, valid_losses, valid_accuracies\n","\n","\n","# K-Fold Cross Validation\n","def k_fold_cross_validation(dataset, model_class, num_folds, num_epochs, batch_size, patience):\n","    kfold = KFold(n_splits=num_folds, shuffle=True)\n","\n","    # To store validation performance and learning curves across folds\n","    all_train_losses, all_valid_losses, all_valid_accuracies = [], [], []\n","\n","    # K-fold Cross Validation\n","    for fold, (train_idx, valid_idx) in enumerate(kfold.split(dataset)):\n","        print(f'Fold {fold + 1}/{num_folds}')\n","\n","        # Subset the data for training and validation\n","        train_subset = Subset(dataset, train_idx)\n","        valid_subset = Subset(dataset, valid_idx)\n","\n","        # Create DataLoaders for this fold\n","        trainloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n","        validloader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False, num_workers=4)\n","\n","        # Create a new instance of the model for each fold\n","        model = model_class()\n","\n","        # Loss function and optimizer\n","        criterion = nn.CrossEntropyLoss()\n","        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-2)\n","\n","        # Train and validate\n","        train_losses, valid_losses, valid_accuracies = train_and_evaluate(\n","            trainloader, validloader, model, criterion, optimizer, num_epochs=num_epochs, patience=patience\n","        )\n","\n","        # Collect learning curves\n","        all_train_losses.append(train_losses)\n","        all_valid_losses.append(valid_losses)\n","        all_valid_accuracies.append(valid_accuracies)\n","\n","    # Return learning curves for further plotting\n","    return all_train_losses, all_valid_losses, all_valid_accuracies\n"]},{"cell_type":"markdown","metadata":{"id":"ZuNnGimTNKja"},"source":["Load data and run training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"UbxCms8wKXpa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fold 1/5\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/5 (Train): 100%|██████████| 313/313 [10:45\u003c00:00,  2.06s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training: Loss: 0.5320\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/5 (Valid): 100%|██████████| 79/79 [00:19\u003c00:00,  4.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation: Loss: 0.3920\n","Validation: Accuracy: 86.43%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/5 (Train): 100%|██████████| 313/313 [09:27\u003c00:00,  1.81s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training: Loss: 0.2238\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/5 (Valid): 100%|██████████| 79/79 [00:19\u003c00:00,  4.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation: Loss: 0.3420\n","Validation: Accuracy: 88.73%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3/5 (Train): 100%|██████████| 313/313 [09:28\u003c00:00,  1.82s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training: Loss: 0.1494\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3/5 (Valid): 100%|██████████| 79/79 [00:18\u003c00:00,  4.16it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation: Loss: 0.2910\n","Validation: Accuracy: 90.60%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4/5 (Train): 100%|██████████| 313/313 [09:31\u003c00:00,  1.83s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training: Loss: 0.1156\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4/5 (Valid): 100%|██████████| 79/79 [00:19\u003c00:00,  4.15it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation: Loss: 0.3300\n","Validation: Accuracy: 89.95%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5/5 (Train): 100%|██████████| 313/313 [09:27\u003c00:00,  1.81s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training: Loss: 0.0934\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5/5 (Valid): 100%|██████████| 79/79 [00:18\u003c00:00,  4.16it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation: Loss: 0.3087\n","Validation: Accuracy: 90.25%\n","Early stopping at epoch 5\n","Fold 2/5\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/5 (Train): 100%|██████████| 313/313 [09:34\u003c00:00,  1.83s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training: Loss: 0.5362\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/5 (Valid): 100%|██████████| 79/79 [00:19\u003c00:00,  4.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation: Loss: 0.4371\n","Validation: Accuracy: 85.02%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/5 (Train): 100%|██████████| 313/313 [09:23\u003c00:00,  1.80s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training: Loss: 0.2266\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/5 (Valid): 100%|██████████| 79/79 [00:19\u003c00:00,  4.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation: Loss: 0.3554\n","Validation: Accuracy: 87.95%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3/5 (Train): 100%|██████████| 313/313 [09:28\u003c00:00,  1.82s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training: Loss: 0.1544\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3/5 (Valid): 100%|██████████| 79/79 [00:19\u003c00:00,  4.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation: Loss: 0.3302\n","Validation: Accuracy: 89.34%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4/5 (Train): 100%|██████████| 313/313 [09:27\u003c00:00,  1.81s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training: Loss: 0.1201\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4/5 (Valid): 100%|██████████| 79/79 [00:19\u003c00:00,  4.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation: Loss: 0.2937\n","Validation: Accuracy: 90.54%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5/5 (Train): 100%|██████████| 313/313 [09:25\u003c00:00,  1.81s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training: Loss: 0.0954\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5/5 (Valid): 100%|██████████| 79/79 [00:19\u003c00:00,  4.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation: Loss: 0.2606\n","Validation: Accuracy: 91.75%\n","Fold 3/5\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/5 (Train): 100%|██████████| 313/313 [09:26\u003c00:00,  1.81s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training: Loss: 0.5276\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/5 (Valid): 100%|██████████| 79/79 [00:19\u003c00:00,  4.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation: Loss: 0.3914\n","Validation: Accuracy: 86.76%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/5 (Train): 100%|██████████| 313/313 [09:43\u003c00:00,  1.86s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training: Loss: 0.2216\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/5 (Valid): 100%|██████████| 79/79 [00:19\u003c00:00,  4.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation: Loss: 0.3794\n","Validation: Accuracy: 87.66%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3/5 (Train): 100%|██████████| 313/313 [09:34\u003c00:00,  1.84s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training: Loss: 0.1527\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3/5 (Valid): 100%|██████████| 79/79 [00:19\u003c00:00,  4.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation: Loss: 0.3306\n","Validation: Accuracy: 89.33%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4/5 (Train): 100%|██████████| 313/313 [09:31\u003c00:00,  1.82s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training: Loss: 0.1149\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4/5 (Valid): 100%|██████████| 79/79 [00:19\u003c00:00,  4.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation: Loss: 0.2794\n","Validation: Accuracy: 91.05%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5/5 (Train): 100%|██████████| 313/313 [09:36\u003c00:00,  1.84s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training: Loss: 0.0932\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5/5 (Valid): 100%|██████████| 79/79 [00:19\u003c00:00,  4.11it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation: Loss: 0.3304\n","Validation: Accuracy: 90.50%\n","Fold 4/5\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/5 (Train): 100%|██████████| 313/313 [09:41\u003c00:00,  1.86s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training: Loss: 0.5159\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/5 (Valid): 100%|██████████| 79/79 [00:19\u003c00:00,  4.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation: Loss: 0.3801\n","Validation: Accuracy: 87.08%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/5 (Train): 100%|██████████| 313/313 [09:31\u003c00:00,  1.83s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training: Loss: 0.2203\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/5 (Valid): 100%|██████████| 79/79 [00:19\u003c00:00,  4.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation: Loss: 0.3031\n","Validation: Accuracy: 89.82%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3/5 (Train): 100%|██████████| 313/313 [09:34\u003c00:00,  1.84s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training: Loss: 0.1550\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3/5 (Valid): 100%|██████████| 79/79 [00:19\u003c00:00,  4.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation: Loss: 0.3154\n","Validation: Accuracy: 89.56%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4/5 (Train): 100%|██████████| 313/313 [09:31\u003c00:00,  1.82s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training: Loss: 0.1186\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4/5 (Valid): 100%|██████████| 79/79 [00:19\u003c00:00,  4.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation: Loss: 0.2686\n","Validation: Accuracy: 91.11%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5/5 (Train): 100%|██████████| 313/313 [09:30\u003c00:00,  1.82s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training: Loss: 0.0933\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5/5 (Valid): 100%|██████████| 79/79 [00:19\u003c00:00,  4.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation: Loss: 0.3183\n","Validation: Accuracy: 90.43%\n","Fold 5/5\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/5 (Train): 100%|██████████| 313/313 [09:32\u003c00:00,  1.83s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training: Loss: 0.5416\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/5 (Valid): 100%|██████████| 79/79 [00:19\u003c00:00,  4.11it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation: Loss: 0.3298\n","Validation: Accuracy: 88.71%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/5 (Train):  20%|██        | 64/313 [01:59\u003c05:31,  1.33s/it]"]}],"source":["# Path to the dataset in Google Drive\n","data_path = '/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/cifar-10-python/cifar-10-batches-py/'\n","\n","# Training and test batch file names\n","train_batches = [f'data_batch_{i}' for i in range(1, 6)]\n","test_batches = ['test_batch']\n","\n","# Define the necessary transforms\n","transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((224, 224)),  # ResNet-50 requires 224x224 input size\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n","])\n","\n","# Create Dataset instance for the full training dataset\n","train_dataset = CIFAR10Dataset(data_path, train_batches, transform=transform)\n","\n","# Perform 5-fold cross-validation on the dataset with ResNet-50 and save the losses\n","train_losses, valid_losses = k_fold_cross_validation(train_dataset, lambda: resnet50(weights=ResNet50_Weights.DEFAULT), num_folds=5, num_epochs=5, batch_size=128, patience=2)"]},{"cell_type":"markdown","metadata":{"id":"pdzUrP2GNP6q"},"source":["Evaluation on test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7iC9hDLuDeOC"},"outputs":[],"source":["def test_model_with_confusion_matrix(model, testloader):\n","    model.eval()  # Set model to evaluation mode\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        progress_bar = tqdm(enumerate(testloader), total=len(testloader), desc='Testing')\n","        for i, (inputs, labels) in progress_bar:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs, 1)\n","\n","            # Append predictions and labels for confusion matrix and metrics\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    # Generate the confusion matrix\n","    cm = confusion_matrix(all_labels, all_preds)\n","    print(f\"Confusion Matrix:\\n{cm}\")\n","\n","    # Calculate metrics\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    precision = precision_score(all_labels, all_preds, average='weighted')\n","    recall = recall_score(all_labels, all_preds, average='weighted')\n","    f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","    # Print the computed metrics\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1 Score: {f1:.4f}\")\n","\n","    # Plot confusion matrix using seaborn\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[i for i in range(10)], yticklabels=[i for i in range(10)])\n","    plt.ylabel('Actual')\n","    plt.xlabel('Predicted')\n","    plt.title('Confusion Matrix')\n","    plt.show()\n","\n","# Test the model and display confusion matrix and metrics\n","test_model_with_confusion_matrix(model, testloader)"]},{"cell_type":"markdown","metadata":{"id":"hbucirhENW8h"},"source":["Plotting learning curves"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j6TIN-46KLki"},"outputs":[],"source":["def plot_learning_curves(train_losses, valid_losses):\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(train_losses, label='Training Loss')\n","    plt.plot(valid_losses, label='Validation Loss')\n","    plt.plot(valid_accuracies, label='Validation Accuracy')\n","    plt.title('Learning Curves')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss' if 'Loss' in train_losses[0] else 'Accuracy')\n","    plt.legend()\n","    plt.show()\n","\n","plot_learning_curves(train_losses, valid_losses)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOUmbU9TV9w7qs5mPpP0FZ2","gpuType":"L4","machine_shape":"hm","mount_file_id":"1t4AfRAtrcZCYkPgmvZGqtalrNYpumWww","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}