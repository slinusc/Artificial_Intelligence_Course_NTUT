{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "eNA59NAbUomN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Define Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_dim, img_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(noise_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, img_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Define Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(img_dim, 1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(1024, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Gradient Penalty for stabilization\n",
        "def gradient_penalty(discriminator, real, fake):\n",
        "    batch_size, img_dim = real.size()\n",
        "    epsilon = torch.rand(batch_size, 1).repeat(1, img_dim).to(\"cuda\")\n",
        "    interpolated = (epsilon * real + (1 - epsilon) * fake).requires_grad_(True)\n",
        "    prob_interpolated = discriminator(interpolated)\n",
        "    gradients = torch.autograd.grad(outputs=prob_interpolated, inputs=interpolated,\n",
        "                                    grad_outputs=torch.ones_like(prob_interpolated),\n",
        "                                    create_graph=True, retain_graph=True)[0]\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    return penalty\n",
        "\n",
        "# Hyperparameters\n",
        "noise_dim = 52*52\n",
        "img_dim = 52 * 52\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "lr_gen = 0.0002\n",
        "lr_disc = 0.00001\n",
        "label_smoothing_real = 0.9\n",
        "label_smoothing_fake = 0.1\n",
        "grad_penalty_lambda = 10  # Coefficient for gradient penalty\n",
        "\n",
        "# Dataset Loader for MixedWM38\n",
        "class WaferMapDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        with np.load(file_path) as data:\n",
        "            print(data)\n",
        "            self.images = data['arr_0']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx].astype(np.float32).flatten()\n",
        "        return img\n",
        "\n"
      ],
      "metadata": {
        "id": "oe86_IBkvN86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "dataset = WaferMapDataset(file_path=\"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/Wafer_Map_Datasets.npz\")\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "IPbIN6EYAceo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZVuzs9VTpbr"
      },
      "outputs": [],
      "source": [
        "# Initialize models, optimizers, and loss\n",
        "generator = Generator(noise_dim, img_dim).to(\"cuda\")\n",
        "discriminator = Discriminator(img_dim).to(\"cuda\")\n",
        "optim_gen = optim.Adam(generator.parameters(), lr=lr_gen)\n",
        "optim_disc = optim.Adam(discriminator.parameters(), lr=lr_disc)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Track losses\n",
        "losses_gen = []\n",
        "losses_disc = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    for real in loader:\n",
        "        real = real.to(\"cuda\")\n",
        "        real += 0.05 * torch.randn_like(real) # A bit of noise to make it harder for discriminator\n",
        "        batch_size = real.size(0)\n",
        "\n",
        "        # Add label flipping\n",
        "        flip_real = torch.rand(batch_size) < 0.1  # 10% chance to flip labels\n",
        "        flip_fake = torch.rand(batch_size) < 0.1\n",
        "\n",
        "        # Train Discriminator\n",
        "        noise = torch.randn(batch_size, noise_dim).to(\"cuda\")\n",
        "        fake = generator(noise)\n",
        "        disc_real = discriminator(real).view(-1)\n",
        "        real_labels = torch.full_like(disc_real, label_smoothing_real)\n",
        "        real_labels[flip_real] = label_smoothing_fake  # Flip some real labels\n",
        "        loss_real = criterion(disc_real, real_labels)\n",
        "\n",
        "        disc_fake = discriminator(fake.detach()).view(-1)\n",
        "        fake_labels = torch.full_like(disc_fake, label_smoothing_fake)\n",
        "        fake_labels[flip_fake] = label_smoothing_real  # Flip some fake labels\n",
        "        loss_fake = criterion(disc_fake, fake_labels)\n",
        "\n",
        "        gp = gradient_penalty(discriminator, real, fake)  # Apply gradient penalty\n",
        "        loss_disc = (loss_real + loss_fake) / 2 + grad_penalty_lambda * gp\n",
        "\n",
        "        optim_disc.zero_grad()\n",
        "        loss_disc.backward()\n",
        "        optim_disc.step()\n",
        "\n",
        "        # Train Generator (2 as frequently)\n",
        "        for _ in range(2):\n",
        "            noise = torch.randn(batch_size, noise_dim).to(\"cuda\")\n",
        "            fake = generator(noise)\n",
        "            disc_fake = discriminator(fake).view(-1)\n",
        "            loss_gen = criterion(disc_fake, torch.ones_like(disc_fake))\n",
        "            optim_gen.zero_grad()\n",
        "            loss_gen.backward()\n",
        "            optim_gen.step()\n",
        "\n",
        "    # Log losses\n",
        "    losses_gen.append(loss_gen.item())\n",
        "    losses_disc.append(loss_disc.item())\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] Loss D: {loss_disc:.4f}, Loss G: {loss_gen:.4f}\")\n",
        "\n",
        "    # Save generated images\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        save_image(fake.view(-1, 1, 52, 52), f\"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/output/enhanced_fake_{epoch+1}.png\")\n",
        "        # Plot learning curves\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(losses_gen, label='Generator Loss')\n",
        "        plt.plot(losses_disc, label='Discriminator Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.title('Learning Curve')\n",
        "        plt.savefig(\"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/output/enhanced_learning_curve.png\")\n",
        "        plt.close()\n",
        "# Save final model\n",
        "torch.save(generator.state_dict(), \"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/models/enhanced_generator.pth\")\n",
        "torch.save(discriminator.state_dict(), \"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/models/enhanced_discriminator.pth\")\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(losses_gen, label='Generator Loss')\n",
        "plt.plot(losses_disc, label='Discriminator Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Learning Curve')\n",
        "plt.savefig(\"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/output/enhanced_learning_curve.png\")\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Evaluate MSE\n",
        "def calculate_mse(real_images, fake_images):\n",
        "    real_images_flat = real_images.view(real_images.size(0), -1).cpu().detach().numpy()\n",
        "    fake_images_flat = fake_images.view(fake_images.size(0), -1).cpu().detach().numpy()\n",
        "    mse = mean_squared_error(real_images_flat, fake_images_flat)\n",
        "    return mse\n",
        "\n",
        "# Generate samples for MSE calculation\n",
        "real_samples, _ = next(iter(loader))  # Get a batch of real samples\n",
        "real_samples = real_samples.to(\"cuda\")\n",
        "\n",
        "noise = torch.randn(batch_size, noise_dim).to(\"cuda\")\n",
        "generated_samples = generator(noise)\n",
        "\n",
        "# Calculate MSE\n",
        "mse = calculate_mse(real_samples, generated_samples)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")"
      ],
      "metadata": {
        "id": "ynpkOxtZZg1T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}