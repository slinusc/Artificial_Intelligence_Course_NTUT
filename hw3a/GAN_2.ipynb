{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNA59NAbUomN",
        "outputId": "c303456b-3bb2-4ee2-ea13-f9a4b9652d52"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Define Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_dim, img_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(noise_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, img_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Define Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(img_dim, 1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(1024, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Gradient Penalty for stabilization\n",
        "def gradient_penalty(discriminator, real, fake):\n",
        "    batch_size, img_dim = real.size()\n",
        "    epsilon = torch.rand(batch_size, 1).repeat(1, img_dim).to(\"cuda\")\n",
        "    interpolated = (epsilon * real + (1 - epsilon) * fake).requires_grad_(True)\n",
        "    prob_interpolated = discriminator(interpolated)\n",
        "    gradients = torch.autograd.grad(outputs=prob_interpolated, inputs=interpolated,\n",
        "                                    grad_outputs=torch.ones_like(prob_interpolated),\n",
        "                                    create_graph=True, retain_graph=True)[0]\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    return penalty\n",
        "\n",
        "# Hyperparameters\n",
        "noise_dim = 52*52  # Increased noise dimensionality\n",
        "img_dim = 52 * 52  # Adjusted to match dataset size\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "lr_gen = 0.0002\n",
        "lr_disc = 0.00001\n",
        "label_smoothing_real = 0.9\n",
        "label_smoothing_fake = 0.1\n",
        "grad_penalty_lambda = 10  # Coefficient for gradient penalty\n",
        "\n",
        "# Dataset Loader for MixedWM38\n",
        "class WaferMapDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        with np.load(file_path) as data:\n",
        "            print(data)\n",
        "            self.images = data['arr_0']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx].astype(np.float32).flatten()\n",
        "        return img\n",
        "\n"
      ],
      "metadata": {
        "id": "oe86_IBkvN86"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "dataset = WaferMapDataset(file_path=\"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/Wafer_Map_Datasets.npz\")\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPbIN6EYAceo",
        "outputId": "6beebd22-d3c0-4f77-f12c-080d45707e82"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NpzFile '/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/Wafer_Map_Datasets.npz' with keys: arr_0, arr_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oZVuzs9VTpbr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "outputId": "66f619bc-9fed-43a9-8ed2-7d11f13bc65b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100] Loss D: 5.7369, Loss G: 0.0002\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-32c4af9552c7>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0moptim_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mloss_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0moptim_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Log losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    221\u001b[0m             )\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlerp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Initialize models, optimizers, and loss\n",
        "generator = Generator(noise_dim, img_dim).to(\"cuda\")\n",
        "discriminator = Discriminator(img_dim).to(\"cuda\")\n",
        "optim_gen = optim.Adam(generator.parameters(), lr=lr_gen)\n",
        "optim_disc = optim.Adam(discriminator.parameters(), lr=lr_disc)\n",
        "criterion = nn.BCEWithLogitsLoss()  # Updated to match non-sigmoid output\n",
        "\n",
        "# Track losses\n",
        "losses_gen = []\n",
        "losses_disc = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    for real in loader:\n",
        "        real = real.to(\"cuda\")\n",
        "        real += 0.05 * torch.randn_like(real) # A bit of noise to make it harder for discriminator\n",
        "        batch_size = real.size(0)\n",
        "\n",
        "        # Add label flipping\n",
        "        flip_real = torch.rand(batch_size) < 0.1  # 10% chance to flip labels\n",
        "        flip_fake = torch.rand(batch_size) < 0.1\n",
        "\n",
        "        # Train Discriminator\n",
        "        noise = torch.randn(batch_size, noise_dim).to(\"cuda\")\n",
        "        fake = generator(noise)\n",
        "        disc_real = discriminator(real).view(-1)\n",
        "        real_labels = torch.full_like(disc_real, label_smoothing_real)\n",
        "        real_labels[flip_real] = label_smoothing_fake  # Flip some real labels\n",
        "        loss_real = criterion(disc_real, real_labels)\n",
        "\n",
        "        disc_fake = discriminator(fake.detach()).view(-1)\n",
        "        fake_labels = torch.full_like(disc_fake, label_smoothing_fake)\n",
        "        fake_labels[flip_fake] = label_smoothing_real  # Flip some fake labels\n",
        "        loss_fake = criterion(disc_fake, fake_labels)\n",
        "\n",
        "        gp = gradient_penalty(discriminator, real, fake)  # Apply gradient penalty\n",
        "        loss_disc = (loss_real + loss_fake) / 2 + grad_penalty_lambda * gp\n",
        "\n",
        "        optim_disc.zero_grad()\n",
        "        loss_disc.backward()\n",
        "        optim_disc.step()\n",
        "\n",
        "        # Train Generator (2 as frequently)\n",
        "        for _ in range(2):\n",
        "            noise = torch.randn(batch_size, noise_dim).to(\"cuda\")\n",
        "            fake = generator(noise)\n",
        "            disc_fake = discriminator(fake).view(-1)\n",
        "            loss_gen = criterion(disc_fake, torch.ones_like(disc_fake))\n",
        "            optim_gen.zero_grad()\n",
        "            loss_gen.backward()\n",
        "            optim_gen.step()\n",
        "\n",
        "    # Log losses\n",
        "    losses_gen.append(loss_gen.item())\n",
        "    losses_disc.append(loss_disc.item())\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] Loss D: {loss_disc:.4f}, Loss G: {loss_gen:.4f}\")\n",
        "\n",
        "    # Save generated images\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        save_image(fake.view(-1, 1, 52, 52), f\"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/output/enhanced_fake_{epoch+1}.png\")\n",
        "        # Plot learning curves\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(losses_gen, label='Generator Loss')\n",
        "        plt.plot(losses_disc, label='Discriminator Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.title('Learning Curve')\n",
        "        plt.savefig(\"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/output/enhanced_learning_curve.png\")\n",
        "        plt.close()\n",
        "# Save final model\n",
        "torch.save(generator.state_dict(), \"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/models/enhanced_generator.pth\")\n",
        "torch.save(discriminator.state_dict(), \"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/models/enhanced_discriminator.pth\")\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(losses_gen, label='Generator Loss')\n",
        "plt.plot(losses_disc, label='Discriminator Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Learning Curve')\n",
        "plt.savefig(\"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/output/enhanced_learning_curve.png\")\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Evaluate MSE\n",
        "def calculate_mse(real_images, fake_images):\n",
        "    real_images_flat = real_images.view(real_images.size(0), -1).cpu().detach().numpy()\n",
        "    fake_images_flat = fake_images.view(fake_images.size(0), -1).cpu().detach().numpy()\n",
        "    mse = mean_squared_error(real_images_flat, fake_images_flat)\n",
        "    return mse\n",
        "\n",
        "# Generate samples for MSE calculation\n",
        "real_samples, _ = next(iter(loader))  # Get a batch of real samples\n",
        "real_samples = real_samples.to(\"cuda\")\n",
        "\n",
        "noise = torch.randn(batch_size, noise_dim).to(\"cuda\")\n",
        "generated_samples = generator(noise)\n",
        "\n",
        "# Calculate MSE\n",
        "mse = calculate_mse(real_samples, generated_samples)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")"
      ],
      "metadata": {
        "id": "ynpkOxtZZg1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OVpFdJYOZf1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eVt6iLZcAbOG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}