{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eNA59NAbUomN","outputId":"90739ec3-fa97-4f45-dbce-f2c14ebc81c4","executionInfo":{"status":"ok","timestamp":1734485916990,"user_tz":-480,"elapsed":26497,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision.utils import save_image\n","from torch.utils.data import DataLoader, Dataset\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","\n","# Define Generator\n","class Generator(nn.Module):\n","    def __init__(self, noise_dim, img_dim):\n","        super(Generator, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(noise_dim, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 1024),\n","            nn.ReLU(),\n","            nn.Linear(1024, img_dim),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# Define Discriminator\n","class Discriminator(nn.Module):\n","    def __init__(self, img_dim):\n","        super(Discriminator, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(img_dim, 1024),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","            nn.Linear(1024, 1)\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# Gradient Penalty for stabilization\n","def gradient_penalty(discriminator, real, fake):\n","    batch_size, img_dim = real.size()\n","    epsilon = torch.rand(batch_size, 1).repeat(1, img_dim).to(\"cuda\")\n","    interpolated = (epsilon * real + (1 - epsilon) * fake).requires_grad_(True)\n","    prob_interpolated = discriminator(interpolated)\n","    gradients = torch.autograd.grad(outputs=prob_interpolated, inputs=interpolated,\n","                                    grad_outputs=torch.ones_like(prob_interpolated),\n","                                    create_graph=True, retain_graph=True)[0]\n","    gradients = gradients.view(gradients.size(0), -1)\n","    penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n","    return penalty\n","\n","# Hyperparameters\n","noise_dim = 52*52  # Increased noise dimensionality\n","img_dim = 52 * 52  # Adjusted to match dataset size\n","batch_size = 64\n","epochs = 100\n","lr_gen = 0.0002\n","lr_disc = 0.00001\n","label_smoothing_real = 0.9\n","label_smoothing_fake = 0.1\n","grad_penalty_lambda = 10  # Coefficient for gradient penalty\n","\n","# Dataset Loader for MixedWM38\n","class WaferMapDataset(Dataset):\n","    def __init__(self, file_path):\n","        with np.load(file_path) as data:\n","            print(data)\n","            self.images = data['arr_0']\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img = self.images[idx].astype(np.float32).flatten()\n","        return img"],"metadata":{"id":"oe86_IBkvN86","executionInfo":{"status":"ok","timestamp":1734485924963,"user_tz":-480,"elapsed":7977,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Load dataset\n","dataset = WaferMapDataset(file_path=\"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/Wafer_Map_Datasets.npz\")\n","loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IPbIN6EYAceo","outputId":"29874823-ad9e-40c3-ccd8-11d894c71bbc","executionInfo":{"status":"ok","timestamp":1734485932001,"user_tz":-480,"elapsed":7043,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["NpzFile '/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/Wafer_Map_Datasets.npz' with keys: arr_0, arr_1\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"oZVuzs9VTpbr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"addd43d1-024f-45de-a298-cd8e1a47e85b","executionInfo":{"status":"ok","timestamp":1734486833063,"user_tz":-480,"elapsed":901071,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)\n","  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100] Loss D: 5.9634, Loss G: 0.0002\n","Epoch [2/100] Loss D: 3.1626, Loss G: 0.0186\n","Epoch [3/100] Loss D: 0.6765, Loss G: 0.9394\n","Epoch [4/100] Loss D: 0.5303, Loss G: 1.8892\n","Epoch [5/100] Loss D: 0.5423, Loss G: 1.5189\n","Epoch [6/100] Loss D: 0.5548, Loss G: 1.7150\n","Epoch [7/100] Loss D: 0.5794, Loss G: 1.7488\n","Epoch [8/100] Loss D: 0.5139, Loss G: 1.7209\n","Epoch [9/100] Loss D: 0.5931, Loss G: 1.6538\n","Epoch [10/100] Loss D: 0.5285, Loss G: 1.5946\n","Epoch [11/100] Loss D: 0.7888, Loss G: 1.7998\n","Epoch [12/100] Loss D: 0.5689, Loss G: 1.9478\n","Epoch [13/100] Loss D: 0.6247, Loss G: 1.8325\n","Epoch [14/100] Loss D: 0.6378, Loss G: 1.9408\n","Epoch [15/100] Loss D: 0.7025, Loss G: 1.8889\n","Epoch [16/100] Loss D: 0.6064, Loss G: 1.7812\n","Epoch [17/100] Loss D: 0.5286, Loss G: 1.9875\n","Epoch [18/100] Loss D: 0.4631, Loss G: 2.0959\n","Epoch [19/100] Loss D: 0.4754, Loss G: 1.6534\n","Epoch [20/100] Loss D: 0.5345, Loss G: 1.6896\n","Epoch [21/100] Loss D: 0.4589, Loss G: 1.6783\n","Epoch [22/100] Loss D: 0.5214, Loss G: 1.6947\n","Epoch [23/100] Loss D: 0.4391, Loss G: 1.5755\n","Epoch [24/100] Loss D: 0.5220, Loss G: 1.6121\n","Epoch [25/100] Loss D: 0.5128, Loss G: 1.6849\n","Epoch [26/100] Loss D: 0.4986, Loss G: 1.7650\n","Epoch [27/100] Loss D: 0.4542, Loss G: 1.8385\n","Epoch [28/100] Loss D: 0.4711, Loss G: 1.9485\n","Epoch [29/100] Loss D: 0.4877, Loss G: 1.7552\n","Epoch [30/100] Loss D: 0.5474, Loss G: 1.5860\n","Epoch [31/100] Loss D: 0.4221, Loss G: 1.7900\n","Epoch [32/100] Loss D: 0.4964, Loss G: 1.6345\n","Epoch [33/100] Loss D: 0.4550, Loss G: 1.8603\n","Epoch [34/100] Loss D: 0.4991, Loss G: 1.7760\n","Epoch [35/100] Loss D: 0.4439, Loss G: 1.8821\n","Epoch [36/100] Loss D: 0.4915, Loss G: 1.6535\n","Epoch [37/100] Loss D: 0.4662, Loss G: 1.5467\n","Epoch [38/100] Loss D: 0.5488, Loss G: 1.6517\n","Epoch [39/100] Loss D: 0.5204, Loss G: 1.8456\n","Epoch [40/100] Loss D: 0.4652, Loss G: 1.6236\n","Epoch [41/100] Loss D: 0.4449, Loss G: 1.8050\n","Epoch [42/100] Loss D: 0.4630, Loss G: 1.7166\n","Epoch [43/100] Loss D: 0.4320, Loss G: 1.8475\n","Epoch [44/100] Loss D: 0.4588, Loss G: 1.7064\n","Epoch [45/100] Loss D: 0.5394, Loss G: 1.6762\n","Epoch [46/100] Loss D: 0.5253, Loss G: 1.6715\n","Epoch [47/100] Loss D: 0.5348, Loss G: 1.6056\n","Epoch [48/100] Loss D: 0.4941, Loss G: 1.7330\n","Epoch [49/100] Loss D: 0.4580, Loss G: 1.6759\n","Epoch [50/100] Loss D: 0.4693, Loss G: 1.7287\n","Epoch [51/100] Loss D: 0.5062, Loss G: 1.6582\n","Epoch [52/100] Loss D: 0.4484, Loss G: 1.6911\n","Epoch [53/100] Loss D: 0.4567, Loss G: 1.7709\n","Epoch [54/100] Loss D: 0.4371, Loss G: 1.8794\n","Epoch [55/100] Loss D: 0.5047, Loss G: 1.7151\n","Epoch [56/100] Loss D: 0.5201, Loss G: 1.7671\n","Epoch [57/100] Loss D: 0.4791, Loss G: 1.6983\n","Epoch [58/100] Loss D: 0.4815, Loss G: 1.7318\n","Epoch [59/100] Loss D: 0.4926, Loss G: 1.7392\n","Epoch [60/100] Loss D: 0.5422, Loss G: 1.7701\n","Epoch [61/100] Loss D: 0.4312, Loss G: 1.7450\n","Epoch [62/100] Loss D: 0.4927, Loss G: 1.7054\n","Epoch [63/100] Loss D: 0.4453, Loss G: 1.8221\n","Epoch [64/100] Loss D: 0.4383, Loss G: 1.7549\n","Epoch [65/100] Loss D: 0.4388, Loss G: 1.7257\n","Epoch [66/100] Loss D: 0.4633, Loss G: 1.7845\n","Epoch [67/100] Loss D: 0.5166, Loss G: 1.5809\n","Epoch [68/100] Loss D: 0.4880, Loss G: 1.7551\n","Epoch [69/100] Loss D: 0.4682, Loss G: 1.7888\n","Epoch [70/100] Loss D: 0.4429, Loss G: 1.6740\n","Epoch [71/100] Loss D: 0.4674, Loss G: 1.6547\n","Epoch [72/100] Loss D: 0.4643, Loss G: 1.7111\n","Epoch [73/100] Loss D: 0.5075, Loss G: 1.8586\n","Epoch [74/100] Loss D: 0.4772, Loss G: 1.8013\n","Epoch [75/100] Loss D: 0.5131, Loss G: 1.6968\n","Epoch [76/100] Loss D: 0.4368, Loss G: 1.7177\n","Epoch [77/100] Loss D: 0.5432, Loss G: 1.6628\n","Epoch [78/100] Loss D: 0.5166, Loss G: 1.6800\n","Epoch [79/100] Loss D: 0.4783, Loss G: 1.8168\n","Epoch [80/100] Loss D: 0.4482, Loss G: 1.8371\n","Epoch [81/100] Loss D: 0.4842, Loss G: 1.6424\n","Epoch [82/100] Loss D: 0.4695, Loss G: 1.7276\n","Epoch [83/100] Loss D: 0.4695, Loss G: 1.5779\n","Epoch [84/100] Loss D: 0.4929, Loss G: 1.8710\n","Epoch [85/100] Loss D: 0.4519, Loss G: 1.7774\n","Epoch [86/100] Loss D: 0.4559, Loss G: 1.7239\n","Epoch [87/100] Loss D: 0.4970, Loss G: 1.7942\n","Epoch [88/100] Loss D: 0.4717, Loss G: 1.8361\n","Epoch [89/100] Loss D: 0.5325, Loss G: 1.8245\n","Epoch [90/100] Loss D: 0.4509, Loss G: 1.6752\n","Epoch [91/100] Loss D: 0.4696, Loss G: 1.7903\n","Epoch [92/100] Loss D: 0.4836, Loss G: 1.7377\n","Epoch [93/100] Loss D: 0.4613, Loss G: 1.6778\n","Epoch [94/100] Loss D: 0.5094, Loss G: 1.7754\n","Epoch [95/100] Loss D: 0.5200, Loss G: 1.6412\n","Epoch [96/100] Loss D: 0.4874, Loss G: 1.8008\n","Epoch [97/100] Loss D: 0.4684, Loss G: 1.8518\n","Epoch [98/100] Loss D: 0.4996, Loss G: 1.5902\n","Epoch [99/100] Loss D: 0.5076, Loss G: 1.6803\n","Epoch [100/100] Loss D: 0.4889, Loss G: 1.5673\n"]}],"source":["# Initialize models, optimizers, and loss\n","generator = Generator(noise_dim, img_dim).to(\"cuda\")\n","discriminator = Discriminator(img_dim).to(\"cuda\")\n","optim_gen = optim.Adam(generator.parameters(), lr=lr_gen)\n","optim_disc = optim.Adam(discriminator.parameters(), lr=lr_disc)\n","criterion = nn.BCEWithLogitsLoss()  # Updated to match non-sigmoid output\n","\n","# Track losses\n","losses_gen = []\n","losses_disc = []\n","\n","# Training loop\n","for epoch in range(epochs):\n","    for real in loader:\n","        real = real.to(\"cuda\")\n","        real += 0.05 * torch.randn_like(real) # A bit of noise to make it harder for discriminator\n","        batch_size = real.size(0)\n","\n","        # Add label flipping\n","        flip_real = torch.rand(batch_size) < 0.1  # 10% chance to flip labels\n","        flip_fake = torch.rand(batch_size) < 0.1\n","\n","        # Train Discriminator\n","        noise = torch.randn(batch_size, noise_dim).to(\"cuda\")\n","        fake = generator(noise)\n","        disc_real = discriminator(real).view(-1)\n","        real_labels = torch.full_like(disc_real, label_smoothing_real)\n","        real_labels[flip_real] = label_smoothing_fake  # Flip some real labels\n","        loss_real = criterion(disc_real, real_labels)\n","\n","        disc_fake = discriminator(fake.detach()).view(-1)\n","        fake_labels = torch.full_like(disc_fake, label_smoothing_fake)\n","        fake_labels[flip_fake] = label_smoothing_real  # Flip some fake labels\n","        loss_fake = criterion(disc_fake, fake_labels)\n","\n","        gp = gradient_penalty(discriminator, real, fake)  # Apply gradient penalty\n","        loss_disc = (loss_real + loss_fake) / 2 + grad_penalty_lambda * gp\n","\n","        optim_disc.zero_grad()\n","        loss_disc.backward()\n","        optim_disc.step()\n","\n","        # Train Generator (2 as frequently)\n","        for _ in range(2):\n","            noise = torch.randn(batch_size, noise_dim).to(\"cuda\")\n","            fake = generator(noise)\n","            disc_fake = discriminator(fake).view(-1)\n","            loss_gen = criterion(disc_fake, torch.ones_like(disc_fake))\n","            optim_gen.zero_grad()\n","            loss_gen.backward()\n","            optim_gen.step()\n","\n","    # Log losses\n","    losses_gen.append(loss_gen.item())\n","    losses_disc.append(loss_disc.item())\n","\n","    print(f\"Epoch [{epoch+1}/{epochs}] Loss D: {loss_disc:.4f}, Loss G: {loss_gen:.4f}\")\n","\n","    # Save generated images\n","    if (epoch + 1) % 10 == 0:\n","        save_image(fake.view(-1, 1, 52, 52), f\"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/output/enhanced_fake_{epoch+1}.png\")\n","        # Plot learning curves\n","        plt.figure(figsize=(10, 5))\n","        plt.plot(losses_gen, label='Generator Loss')\n","        plt.plot(losses_disc, label='Discriminator Loss')\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Loss')\n","        plt.legend()\n","        plt.title('Learning Curve')\n","        plt.savefig(\"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/output/enhanced_learning_curve.png\")\n","        plt.close()\n","# Save final model\n","torch.save(generator.state_dict(), \"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/models/enhanced_generator.pth\")\n","torch.save(discriminator.state_dict(), \"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/models/enhanced_discriminator.pth\")\n","\n","# Plot learning curves\n","plt.figure(figsize=(10, 5))\n","plt.plot(losses_gen, label='Generator Loss')\n","plt.plot(losses_disc, label='Discriminator Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.title('Learning Curve')\n","plt.savefig(\"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/output/enhanced_learning_curve.png\")\n","plt.close()\n"]},{"cell_type":"code","source":["from sklearn.metrics import mean_squared_error\n","\n","# Evaluate MSE\n","def calculate_mse(real_images, fake_images):\n","    real_images_flat = real_images.view(real_images.size(0), -1).cpu().detach().numpy()\n","    fake_images_flat = fake_images.view(fake_images.size(0), -1).cpu().detach().numpy()\n","    mse = mean_squared_error(real_images_flat, fake_images_flat)\n","    return mse\n","\n","# Generate samples for MSE calculation\n","real_samples = next(iter(loader))  # Get a batch of real samples\n","real_samples = real_samples.to(\"cuda\")\n","\n","# Generate fake samples\n","noise = torch.randn(real_samples.size(0), noise_dim).to(\"cuda\")\n","generated_samples = generator(noise)\n","\n","# Calculate MSE\n","mse = calculate_mse(real_samples, generated_samples)\n","print(f\"Mean Squared Error (MSE): {mse}\")"],"metadata":{"id":"ynpkOxtZZg1T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734487023967,"user_tz":-480,"elapsed":521,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"}},"outputId":"997ea420-52ba-4cad-fd77-8a140a554c4f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Squared Error (MSE): 1.8501698970794678\n"]}]}]}