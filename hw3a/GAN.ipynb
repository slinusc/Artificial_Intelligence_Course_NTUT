{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true,"mount_file_id":"1P_uzLczY1sEbYObZ-Zvw1gWC4LMosmXG","authorship_tag":"ABX9TyNcuCMWXHUOk3wPbwdnzdT2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eNA59NAbUomN","executionInfo":{"status":"ok","timestamp":1734428680911,"user_tz":-480,"elapsed":17208,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"}},"outputId":"be344cb7-f9c5-4749-ee52-190eb1ed5f67"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"oZVuzs9VTpbr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734429126976,"user_tz":-480,"elapsed":446069,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"}},"outputId":"0abf6246-d0af-44a8-87b7-6f711380ced2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)\n","  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/50] Loss D: 0.5483, Loss G: 2.1871\n","Epoch [2/50] Loss D: 0.4881, Loss G: 1.9958\n","Epoch [3/50] Loss D: 0.6351, Loss G: 1.2008\n","Epoch [4/50] Loss D: 0.7093, Loss G: 1.0119\n","Epoch [5/50] Loss D: 1.3678, Loss G: 0.7927\n","Epoch [6/50] Loss D: 0.8886, Loss G: 1.2664\n","Epoch [7/50] Loss D: 1.5131, Loss G: 0.3397\n","Epoch [8/50] Loss D: 0.9907, Loss G: 0.5839\n","Epoch [9/50] Loss D: 0.6661, Loss G: 1.2543\n","Epoch [10/50] Loss D: 0.5594, Loss G: 1.6173\n","Epoch [11/50] Loss D: 0.4933, Loss G: 1.9285\n","Epoch [12/50] Loss D: 0.5614, Loss G: 1.3889\n","Epoch [13/50] Loss D: 0.5184, Loss G: 2.5397\n","Epoch [14/50] Loss D: 0.4051, Loss G: 2.0866\n","Epoch [15/50] Loss D: 0.4106, Loss G: 1.7784\n","Epoch [16/50] Loss D: 0.5219, Loss G: 2.2811\n","Epoch [17/50] Loss D: 0.4468, Loss G: 2.2474\n","Epoch [18/50] Loss D: 0.4207, Loss G: 1.8919\n","Epoch [19/50] Loss D: 0.4546, Loss G: 1.9933\n","Epoch [20/50] Loss D: 0.4580, Loss G: 1.8428\n","Epoch [21/50] Loss D: 0.4169, Loss G: 2.0045\n","Epoch [22/50] Loss D: 0.4231, Loss G: 1.9616\n","Epoch [23/50] Loss D: 0.4554, Loss G: 1.7278\n","Epoch [24/50] Loss D: 0.4728, Loss G: 2.2438\n","Epoch [25/50] Loss D: 0.4060, Loss G: 1.6629\n","Epoch [26/50] Loss D: 0.4142, Loss G: 2.0043\n","Epoch [27/50] Loss D: 0.4172, Loss G: 2.1949\n","Epoch [28/50] Loss D: 0.4275, Loss G: 2.3795\n","Epoch [29/50] Loss D: 0.3782, Loss G: 1.8272\n","Epoch [30/50] Loss D: 0.4491, Loss G: 2.2071\n","Epoch [31/50] Loss D: 0.4180, Loss G: 1.5943\n","Epoch [32/50] Loss D: 0.5521, Loss G: 1.7438\n","Epoch [33/50] Loss D: 0.4320, Loss G: 2.0050\n","Epoch [34/50] Loss D: 0.4419, Loss G: 2.3966\n","Epoch [35/50] Loss D: 0.3897, Loss G: 1.8433\n","Epoch [36/50] Loss D: 0.4570, Loss G: 2.0904\n","Epoch [37/50] Loss D: 0.4753, Loss G: 1.8428\n","Epoch [38/50] Loss D: 0.4068, Loss G: 1.8860\n","Epoch [39/50] Loss D: 0.4253, Loss G: 2.4259\n","Epoch [40/50] Loss D: 0.3818, Loss G: 2.3376\n","Epoch [41/50] Loss D: 0.3969, Loss G: 1.8017\n","Epoch [42/50] Loss D: 0.4320, Loss G: 1.9705\n","Epoch [43/50] Loss D: 0.4740, Loss G: 1.9026\n","Epoch [44/50] Loss D: 0.4569, Loss G: 1.7464\n","Epoch [45/50] Loss D: 0.5172, Loss G: 2.2397\n","Epoch [46/50] Loss D: 0.4261, Loss G: 2.0083\n","Epoch [47/50] Loss D: 0.4274, Loss G: 1.8038\n","Epoch [48/50] Loss D: 0.4564, Loss G: 1.7232\n","Epoch [49/50] Loss D: 0.4372, Loss G: 2.3494\n","Epoch [50/50] Loss D: 0.4807, Loss G: 1.8479\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision.utils import save_image\n","from torch.utils.data import DataLoader, Dataset\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","\n","# Define Generator\n","class Generator(nn.Module):\n","    def __init__(self, noise_dim, img_dim):\n","        super(Generator, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(noise_dim, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 1024),\n","            nn.ReLU(),\n","            nn.Linear(1024, img_dim),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# Define Discriminator\n","class Discriminator(nn.Module):\n","    def __init__(self, img_dim):\n","        super(Discriminator, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(img_dim, 1024),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","            nn.Linear(1024, 512),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","            nn.Linear(512, 256),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(256, 1)\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# Gradient Penalty for stabilization\n","def gradient_penalty(discriminator, real, fake):\n","    batch_size, img_dim = real.size()\n","    epsilon = torch.rand(batch_size, 1).repeat(1, img_dim).to(\"cuda\")\n","    interpolated = (epsilon * real + (1 - epsilon) * fake).requires_grad_(True)\n","    prob_interpolated = discriminator(interpolated)\n","    gradients = torch.autograd.grad(outputs=prob_interpolated, inputs=interpolated,\n","                                    grad_outputs=torch.ones_like(prob_interpolated),\n","                                    create_graph=True, retain_graph=True)[0]\n","    gradients = gradients.view(gradients.size(0), -1)\n","    penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n","    return penalty\n","\n","# Hyperparameters\n","noise_dim = 128  # Increased noise dimensionality\n","img_dim = 52 * 52  # Adjusted to match dataset size\n","batch_size = 64\n","epochs = 50\n","lr_gen = 0.0002\n","lr_disc = 0.0001\n","label_smoothing_real = 0.9\n","label_smoothing_fake = 0.1\n","grad_penalty_lambda = 10  # Coefficient for gradient penalty\n","\n","# Dataset Loader for MixedWM38\n","class WaferMapDataset(Dataset):\n","    def __init__(self, file_path):\n","        with np.load(file_path) as data:\n","            self.images = data['arr_0']\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img = self.images[idx].astype(np.float32).flatten()\n","        return img\n","\n","# Load dataset\n","dataset = WaferMapDataset(file_path=\"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/Wafer_Map_Datasets.npz\")\n","loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","# Initialize models, optimizers, and loss\n","generator = Generator(noise_dim, img_dim).to(\"cuda\")\n","discriminator = Discriminator(img_dim).to(\"cuda\")\n","optim_gen = optim.Adam(generator.parameters(), lr=lr_gen)\n","optim_disc = optim.Adam(discriminator.parameters(), lr=lr_disc)\n","criterion = nn.BCEWithLogitsLoss()  # Updated to match non-sigmoid output\n","\n","# Track losses\n","losses_gen = []\n","losses_disc = []\n","\n","# Training loop\n","for epoch in range(epochs):\n","    for real in loader:\n","        real = real.to(\"cuda\")\n","        batch_size = real.size(0)\n","\n","        # Add label flipping\n","        flip_real = torch.rand(batch_size) < 0.05  # 5% chance to flip labels\n","        flip_fake = torch.rand(batch_size) < 0.05\n","\n","        # Train Discriminator\n","        noise = torch.randn(batch_size, noise_dim).to(\"cuda\")\n","        fake = generator(noise)\n","        disc_real = discriminator(real).view(-1)\n","        real_labels = torch.full_like(disc_real, label_smoothing_real)\n","        real_labels[flip_real] = label_smoothing_fake  # Flip some real labels\n","        loss_real = criterion(disc_real, real_labels)\n","\n","        disc_fake = discriminator(fake.detach()).view(-1)\n","        fake_labels = torch.full_like(disc_fake, label_smoothing_fake)\n","        fake_labels[flip_fake] = label_smoothing_real  # Flip some fake labels\n","        loss_fake = criterion(disc_fake, fake_labels)\n","\n","        gp = gradient_penalty(discriminator, real, fake)  # Apply gradient penalty\n","        loss_disc = (loss_real + loss_fake) / 2 + grad_penalty_lambda * gp\n","\n","        optim_disc.zero_grad()\n","        loss_disc.backward()\n","        optim_disc.step()\n","\n","        # Train Generator (twice as frequently)\n","        for _ in range(2):\n","            noise = torch.randn(batch_size, noise_dim).to(\"cuda\")\n","            fake = generator(noise)\n","            disc_fake = discriminator(fake).view(-1)\n","            loss_gen = criterion(disc_fake, torch.ones_like(disc_fake))\n","            optim_gen.zero_grad()\n","            loss_gen.backward()\n","            optim_gen.step()\n","\n","    # Log losses\n","    losses_gen.append(loss_gen.item())\n","    losses_disc.append(loss_disc.item())\n","\n","    print(f\"Epoch [{epoch+1}/{epochs}] Loss D: {loss_disc:.4f}, Loss G: {loss_gen:.4f}\")\n","\n","    # Save generated images\n","    if (epoch + 1) % 10 == 0:\n","        save_image(fake.view(-1, 1, 52, 52), f\"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/output/enhanced_fake_{epoch+1}.png\")\n","\n","# Save final model\n","torch.save(generator.state_dict(), \"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/models/enhanced_generator.pth\")\n","torch.save(discriminator.state_dict(), \"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/models/enhanced_discriminator.pth\")\n","\n","# Plot learning curves\n","plt.figure(figsize=(10, 5))\n","plt.plot(losses_gen, label='Generator Loss')\n","plt.plot(losses_disc, label='Discriminator Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.title('Learning Curve')\n","plt.savefig(\"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/output/enhanced_learning_curve.png\")\n","plt.close()\n"]}]}