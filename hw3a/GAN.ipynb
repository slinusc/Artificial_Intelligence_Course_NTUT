{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1734412769352,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"},"user_tz":-480},"id":"eNA59NAbUomN"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"oZVuzs9VTpbr"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/50] Loss D: -141.6975, Loss G: 33.9487\n","Epoch [2/50] Loss D: -144.7217, Loss G: 10.3861\n","Epoch [3/50] Loss D: -150.3700, Loss G: -29.5455\n","Epoch [4/50] Loss D: -153.1742, Loss G: -43.1910\n","Epoch [5/50] Loss D: -145.6715, Loss G: -49.6991\n","Epoch [6/50] Loss D: -156.2062, Loss G: -54.6608\n","Epoch [7/50] Loss D: -150.1908, Loss G: -65.3483\n","Epoch [8/50] Loss D: -163.0183, Loss G: -61.5171\n","Epoch [9/50] Loss D: -157.3172, Loss G: -67.9827\n","Epoch [10/50] Loss D: -153.1800, Loss G: -81.5960\n","Epoch [11/50] Loss D: -146.5420, Loss G: -67.8680\n","Epoch [12/50] Loss D: -161.9406, Loss G: -75.5958\n","Epoch [13/50] Loss D: -149.2812, Loss G: -82.1329\n","Epoch [14/50] Loss D: -147.2630, Loss G: -75.0279\n","Epoch [15/50] Loss D: -148.8197, Loss G: -73.3200\n","Epoch [16/50] Loss D: -151.3278, Loss G: -78.2941\n","Epoch [17/50] Loss D: -157.0057, Loss G: -73.7788\n","Epoch [18/50] Loss D: -155.1833, Loss G: -80.1763\n","Epoch [19/50] Loss D: -140.5251, Loss G: -60.2681\n","Epoch [20/50] Loss D: -156.9916, Loss G: -73.9149\n","Epoch [21/50] Loss D: -161.1898, Loss G: -93.3073\n","Epoch [22/50] Loss D: -155.8509, Loss G: -89.5550\n","Epoch [23/50] Loss D: -162.6065, Loss G: -81.8699\n","Epoch [24/50] Loss D: -170.5448, Loss G: -95.8696\n","Epoch [25/50] Loss D: -152.2401, Loss G: -96.0427\n","Epoch [26/50] Loss D: -153.3397, Loss G: -75.1384\n","Epoch [27/50] Loss D: -145.5255, Loss G: -99.0404\n","Epoch [28/50] Loss D: -161.6309, Loss G: -83.4474\n","Epoch [29/50] Loss D: -156.4425, Loss G: -88.1223\n","Epoch [30/50] Loss D: -157.8606, Loss G: -77.0420\n","Epoch [31/50] Loss D: -142.2415, Loss G: -61.6477\n","Epoch [32/50] Loss D: -178.9439, Loss G: -38.5609\n","Epoch [33/50] Loss D: -148.4740, Loss G: -43.6773\n","Epoch [34/50] Loss D: -160.0534, Loss G: -41.2963\n","Epoch [35/50] Loss D: -153.8218, Loss G: -35.5788\n","Epoch [36/50] Loss D: -141.6811, Loss G: -25.4012\n","Epoch [37/50] Loss D: -156.1898, Loss G: -45.2179\n","Epoch [38/50] Loss D: -165.8175, Loss G: -25.4776\n","Epoch [39/50] Loss D: -153.0618, Loss G: -7.8151\n","Epoch [40/50] Loss D: -169.8130, Loss G: 36.7508\n","Epoch [41/50] Loss D: -161.0499, Loss G: 9.2628\n","Epoch [42/50] Loss D: -127.6852, Loss G: 0.0268\n","Epoch [43/50] Loss D: -165.1824, Loss G: 9.8168\n","Epoch [44/50] Loss D: -164.4130, Loss G: 32.8709\n","Epoch [45/50] Loss D: -157.1821, Loss G: 38.4368\n","Epoch [46/50] Loss D: -148.3470, Loss G: 51.7198\n","Epoch [47/50] Loss D: -171.5893, Loss G: 56.0184\n","Epoch [48/50] Loss D: -153.7720, Loss G: 37.9916\n","Epoch [49/50] Loss D: -163.7287, Loss G: 49.5066\n","Epoch [50/50] Loss D: -142.0198, Loss G: 18.7865\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms\n","from torchvision.utils import save_image\n","from torch.utils.data import DataLoader, Dataset\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","\n","# Define Generator\n","class Generator(nn.Module):\n","    def __init__(self, noise_dim, img_dim, num_classes):\n","        super(Generator, self).__init__()\n","        self.label_emb = nn.Embedding(num_classes, noise_dim)\n","        self.model = nn.Sequential(\n","            nn.Linear(noise_dim * 2, 256),\n","            nn.BatchNorm1d(256),\n","            nn.ReLU(),\n","            nn.Linear(256, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(),\n","            nn.Linear(512, 1024),\n","            nn.BatchNorm1d(1024),\n","            nn.ReLU(),\n","            nn.Linear(1024, img_dim),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, noise, labels):\n","        label_input = self.label_emb(labels)\n","        x = torch.cat((noise, label_input), dim=1)\n","        return self.model(x)\n","\n","# Define Discriminator\n","class Discriminator(nn.Module):\n","    def __init__(self, img_dim, num_classes):\n","        super(Discriminator, self).__init__()\n","        self.label_emb = nn.Embedding(num_classes, img_dim)\n","        self.model = nn.Sequential(\n","            nn.Linear(img_dim * 2, 1024),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","            nn.Linear(1024, 512),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","            nn.Linear(512, 256),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(256, 1)\n","        )\n","\n","    def forward(self, img, labels):\n","        label_input = self.label_emb(labels)\n","        x = torch.cat((img, label_input), dim=1)\n","        return self.model(x)\n","\n","# Hyperparameters\n","noise_dim = 100\n","img_dim = 52 * 52  # Adjusted to match dataset size\n","num_classes = 8  # Based on defect types\n","batch_size = 128  # Increased batch size\n","epochs = 50\n","lr_gen = 0.00008  # Reduced learning rate for generator\n","lr_disc = 0.00004  # Reduced learning rate for discriminator\n","lambda_gp = 2.0  # Adjusted gradient penalty coefficient\n","num_disc_updates = 2  # Balanced training\n","\n","# Dataset Loader for MixedWM38\n","class WaferMapDataset(Dataset):\n","    def __init__(self, file_path, transform=None):\n","        with np.load(file_path) as data:\n","            self.images = data['arr_0']  # Wafer maps\n","            self.labels = data['arr_1']  # One-hot labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img = self.images[idx].astype(np.float32)\n","        label = np.argmax(self.labels[idx])  # Convert one-hot to class index\n","        if self.transform:\n","            img = self.transform(img)\n","        return img.flatten(), label\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize((52, 52)),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","# Load dataset\n","dataset = WaferMapDataset(file_path=\"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/Wafer_Map_Datasets.npz\", transform=transform)\n","loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","# Initialize models, optimizers, and loss\n","generator = Generator(noise_dim, img_dim, num_classes).to(\"cuda\")\n","discriminator = Discriminator(img_dim, num_classes).to(\"cuda\")\n","optim_gen = optim.Adam(generator.parameters(), lr=lr_gen, betas=(0.5, 0.999))\n","optim_disc = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.999))\n","\n","# Gradient Penalty for stabilization\n","def gradient_penalty(discriminator, real, fake, labels):\n","    batch_size, img_dim = real.size()\n","    epsilon = torch.rand(batch_size, 1).repeat(1, img_dim).to(\"cuda\")\n","    interpolated = (epsilon * real + (1 - epsilon) * fake).requires_grad_(True)\n","    prob_interpolated = discriminator(interpolated, labels)\n","    gradients = torch.autograd.grad(outputs=prob_interpolated, inputs=interpolated,\n","                                    grad_outputs=torch.ones_like(prob_interpolated),\n","                                    create_graph=True, retain_graph=True)[0]\n","    gradients = gradients.view(gradients.size(0), -1)\n","    penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n","    return penalty\n","\n","# Training loop\n","for epoch in range(epochs):\n","    for i, (real, labels) in enumerate(loader):\n","        real, labels = real.to(\"cuda\"), labels.to(\"cuda\")\n","        batch_size = real.size(0)\n","\n","        # Train Discriminator\n","        for _ in range(num_disc_updates):\n","            noise = torch.randn(batch_size, noise_dim).to(\"cuda\")\n","            fake = generator(noise, labels)\n","            disc_real = discriminator(real, labels).view(-1)\n","            disc_fake = discriminator(fake.detach(), labels).view(-1)\n","            gp = gradient_penalty(discriminator, real, fake, labels)\n","            loss_disc = -torch.mean(disc_real) + torch.mean(disc_fake) + lambda_gp * gp\n","            optim_disc.zero_grad()\n","            loss_disc.backward()\n","            nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=10)  # Clip gradients\n","            optim_disc.step()\n","\n","        # Train Generator\n","        noise = torch.randn(batch_size, noise_dim).to(\"cuda\")\n","        fake = generator(noise, labels)\n","        disc_fake = discriminator(fake, labels).view(-1)\n","        loss_gen = -torch.mean(disc_fake)\n","        optim_gen.zero_grad()\n","        loss_gen.backward()\n","        nn.utils.clip_grad_norm_(generator.parameters(), max_norm=10)  # Clip gradients\n","        optim_gen.step()\n","\n","    print(f\"Epoch [{epoch+1}/{epochs}] Loss D: {loss_disc:.4f}, Loss G: {loss_gen:.4f}\")\n","\n","    # Save generated images\n","    if (epoch + 1) % 10 == 0:\n","        save_image(fake.view(-1, 1, 52, 52), f\"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/output/fake_{epoch+1}.png\")\n","\n","# Save final model\n","torch.save(generator.state_dict(), \"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/models/generator.pth\")\n","torch.save(discriminator.state_dict(), \"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/models/discriminator.pth\")"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMnHkFonsn57C7/VBwtuYtC","gpuType":"T4","mount_file_id":"1P_uzLczY1sEbYObZ-Zvw1gWC4LMosmXG","name":"","toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}