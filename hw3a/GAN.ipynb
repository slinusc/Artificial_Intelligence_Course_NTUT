{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm","mount_file_id":"1P_uzLczY1sEbYObZ-Zvw1gWC4LMosmXG","authorship_tag":"ABX9TyMDWLCPcsv6VElwHcOOyb26"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eNA59NAbUomN","executionInfo":{"status":"ok","timestamp":1734499575359,"user_tz":-480,"elapsed":10023,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"}},"outputId":"8ea6f57d-892c-4fd3-8e95-91fb8204489d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oZVuzs9VTpbr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f55082e2-90a3-4802-c1a9-285f06d99180"},"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1/50: 100%|██████████| 38/38 [00:17<00:00,  2.22batch/s, Loss D=0.534, Loss G=2.45]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/50] Loss D: 0.5343, Loss G: 2.4461\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/50: 100%|██████████| 38/38 [00:14<00:00,  2.59batch/s, Loss D=0.547, Loss G=1.82]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [2/50] Loss D: 0.5465, Loss G: 1.8212\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/50: 100%|██████████| 38/38 [00:14<00:00,  2.58batch/s, Loss D=0.495, Loss G=2.01]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [3/50] Loss D: 0.4955, Loss G: 2.0114\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/50: 100%|██████████| 38/38 [00:14<00:00,  2.59batch/s, Loss D=0.514, Loss G=0.957]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [4/50] Loss D: 0.5142, Loss G: 0.9575\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/50: 100%|██████████| 38/38 [00:14<00:00,  2.58batch/s, Loss D=0.516, Loss G=1.1]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [5/50] Loss D: 0.5164, Loss G: 1.0957\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/50: 100%|██████████| 38/38 [00:14<00:00,  2.59batch/s, Loss D=0.505, Loss G=1.24]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [6/50] Loss D: 0.5054, Loss G: 1.2416\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/50: 100%|██████████| 38/38 [00:14<00:00,  2.59batch/s, Loss D=0.521, Loss G=1.23]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [7/50] Loss D: 0.5213, Loss G: 1.2330\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/50: 100%|██████████| 38/38 [00:14<00:00,  2.59batch/s, Loss D=0.477, Loss G=2.05]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [8/50] Loss D: 0.4773, Loss G: 2.0517\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/50: 100%|██████████| 38/38 [00:14<00:00,  2.60batch/s, Loss D=0.484, Loss G=1.86]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [9/50] Loss D: 0.4844, Loss G: 1.8585\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/50: 100%|██████████| 38/38 [00:14<00:00,  2.60batch/s, Loss D=0.471, Loss G=1.6]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [10/50] Loss D: 0.4708, Loss G: 1.5969\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 11/50: 100%|██████████| 38/38 [00:14<00:00,  2.60batch/s, Loss D=0.517, Loss G=2.96]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [11/50] Loss D: 0.5172, Loss G: 2.9576\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 12/50: 100%|██████████| 38/38 [00:14<00:00,  2.59batch/s, Loss D=0.487, Loss G=1.39]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [12/50] Loss D: 0.4875, Loss G: 1.3889\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 13/50: 100%|██████████| 38/38 [00:14<00:00,  2.60batch/s, Loss D=0.507, Loss G=1.79]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [13/50] Loss D: 0.5074, Loss G: 1.7869\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 14/50: 100%|██████████| 38/38 [00:14<00:00,  2.59batch/s, Loss D=0.492, Loss G=1.44]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [14/50] Loss D: 0.4922, Loss G: 1.4404\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 15/50: 100%|██████████| 38/38 [00:14<00:00,  2.58batch/s, Loss D=0.51, Loss G=1.75]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [15/50] Loss D: 0.5099, Loss G: 1.7517\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 16/50: 100%|██████████| 38/38 [00:14<00:00,  2.59batch/s, Loss D=0.469, Loss G=2.39]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [16/50] Loss D: 0.4687, Loss G: 2.3903\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 17/50: 100%|██████████| 38/38 [00:14<00:00,  2.59batch/s, Loss D=0.497, Loss G=1.11]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [17/50] Loss D: 0.4968, Loss G: 1.1130\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 18/50: 100%|██████████| 38/38 [00:14<00:00,  2.58batch/s, Loss D=0.484, Loss G=1.5]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [18/50] Loss D: 0.4842, Loss G: 1.4964\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 19/50: 100%|██████████| 38/38 [00:14<00:00,  2.60batch/s, Loss D=0.444, Loss G=1.95]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [19/50] Loss D: 0.4442, Loss G: 1.9497\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 20/50: 100%|██████████| 38/38 [00:14<00:00,  2.60batch/s, Loss D=0.535, Loss G=2.09]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [20/50] Loss D: 0.5349, Loss G: 2.0940\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 21/50: 100%|██████████| 38/38 [00:14<00:00,  2.60batch/s, Loss D=0.443, Loss G=1.85]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [21/50] Loss D: 0.4429, Loss G: 1.8546\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 22/50: 100%|██████████| 38/38 [00:14<00:00,  2.59batch/s, Loss D=0.519, Loss G=2.01]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [22/50] Loss D: 0.5191, Loss G: 2.0050\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 23/50:  87%|████████▋ | 33/38 [00:13<00:01,  2.55batch/s, Loss D=0.476, Loss G=2.05]"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision.utils import save_image\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","from tqdm import tqdm\n","\n","class Generator(nn.Module):\n","    def __init__(self, noise_dim, num_classes):\n","        super(Generator, self).__init__()\n","        self.label_emb = nn.Embedding(num_classes, noise_dim)\n","        self.model = nn.Sequential(\n","            # Input: [batch_size, noise_dim + noise_dim, 1, 1]\n","            nn.ConvTranspose2d(noise_dim + noise_dim, 512, 4, 1, 0, bias=False),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(True),\n","            # Output: [batch_size, 512, 4, 4]\n","\n","            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(True),\n","            # Output: [batch_size, 256, 8, 8]\n","\n","            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(True),\n","            # Output: [batch_size, 128, 16, 16]\n","\n","            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(True),\n","            # Output: [batch_size, 64, 32, 32]\n","\n","            nn.ConvTranspose2d(64, 1, 4, 2, 1, bias=False),\n","            nn.Tanh()\n","            # Output: [batch_size, 1, 64, 64]\n","        )\n","\n","    def forward(self, noise, labels):\n","        label_input = self.label_emb(labels).unsqueeze(2).unsqueeze(3)\n","        gen_input = torch.cat((noise, label_input), dim=1)\n","        return self.model(gen_input)\n","\n","\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, num_classes):\n","        super(Discriminator, self).__init__()\n","        self.label_emb = nn.Embedding(num_classes, 64 * 64)\n","        self.model = nn.Sequential(\n","            nn.Conv2d(2, 128, 4, 2, 1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(512, 1, kernel_size=8, stride=1, padding=0, bias=False)\n","        )\n","\n","    def forward(self, img, labels):\n","        label_input = self.label_emb(labels).view(labels.size(0), 1, 64, 64)\n","        disc_input = torch.cat((img, label_input), dim=1)\n","        return self.model(disc_input).view(-1)\n","\n","\n","\n","# Hyperparameters\n","noise_dim = 100\n","batch_size = 2048\n","epochs = 50\n","lr_gen = 0.0002\n","lr_disc = 0.0002\n","label_smoothing_real = 0.9\n","label_smoothing_fake = 0.1\n","num_classes = 38\n","\n","# Dataset Loader\n","class WaferMapDataset(Dataset):\n","    def __init__(self, file_path, transform=None):\n","        with np.load(file_path) as data:\n","            self.data = data['arr_0']\n","            self.onehot_labels = data['arr_1']\n","        self.labels = self._get_class_indices(self.onehot_labels)\n","        self.transform = transform\n","\n","    def _get_class_indices(self, onehot_labels):\n","        class_indices = []\n","        for row in onehot_labels:\n","            active_indices = tuple(np.where(row == 1)[0])\n","            if active_indices not in class_indices:\n","                class_indices.append(active_indices)\n","        mapping = {cls: idx for idx, cls in enumerate(class_indices)}\n","        return np.array([mapping[tuple(np.where(row == 1)[0])] for row in onehot_labels])\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img = self.data[idx].astype(np.float32)\n","        label = self.labels[idx]\n","        if self.transform:\n","            img = self.transform(img)\n","        return img, label\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize((64, 64)),\n","    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1] range\n","])\n","\n","# Load dataset\n","dataset = WaferMapDataset(\n","    file_path=\"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/Wafer_Map_Datasets.npz\",\n","    transform=transform\n",")\n","loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","def visualize_real_and_generated(dataset, generator, epoch, num_classes, noise_dim):\n","    fig, axs = plt.subplots(4, 19, figsize=(25, 15))  # 4 rows, 19 columns\n","    axs = axs.flatten()\n","\n","    class_samples = {cls: None for cls in range(num_classes)}\n","\n","    # Collect one sample per class for real data\n","    for img, label in dataset:\n","        if class_samples[label] is None:\n","            class_samples[label] = img.reshape(64, 64)\n","\n","        if all(sample is not None for sample in class_samples.values()):\n","            break\n","\n","    # Generate fake samples\n","    noise = torch.randn(num_classes, noise_dim, 1, 1).to(\"cuda\")\n","    labels = torch.arange(num_classes).to(\"cuda\")\n","    generated = generator(noise, labels).detach().cpu().numpy()\n","\n","    # Plot real samples (top two rows)\n","    for cls in range(num_classes):\n","        axs[cls].imshow(class_samples[cls], cmap=\"Blues\")\n","        axs[cls].set_title(f\"Real Class {cls}\", fontsize=8)\n","        axs[cls].axis(\"off\")\n","\n","    # Plot fake samples (bottom two rows)\n","    for cls in range(num_classes):\n","        axs[cls + 38].imshow(generated[cls].squeeze(), cmap=\"Blues\")\n","        axs[cls + 38].set_title(f\"Fake Class {cls}\", fontsize=8)\n","        axs[cls + 38].axis(\"off\")\n","\n","    plt.tight_layout()\n","    plt.savefig(f\"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/output/epoch_{epoch}_visualization.png\")\n","    plt.close()\n","\n","\n","# Initialize models\n","generator = Generator(noise_dim, num_classes).to(\"cuda\")\n","discriminator = Discriminator(num_classes).to(\"cuda\")\n","optim_gen = optim.Adam(generator.parameters(), lr=lr_gen, betas=(0.5, 0.999))\n","optim_disc = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.999))\n","criterion = nn.BCEWithLogitsLoss()\n","\n","# Track losses\n","losses_gen = []\n","losses_disc = []\n","\n","# Training loop\n","for epoch in range(epochs):\n","    # Progress bar for batches\n","    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\")\n","    for real, labels in pbar:\n","        # Move data to the device\n","        real = real.view(real.size(0), 1, 64, 64).to(\"cuda\")\n","        labels = labels.to(\"cuda\")\n","\n","        # Add noise to the real images for robustness\n","        real += 0.05 * torch.randn_like(real)\n","        batch_size = real.size(0)\n","\n","        # Generate fake images\n","        noise = torch.randn(batch_size, noise_dim, 1, 1).to(\"cuda\")\n","        fake = generator(noise, labels)\n","\n","        # Train Discriminator\n","        flip_real = torch.rand(batch_size, device=real.device) < 0.1\n","        real_labels = torch.full((batch_size,), label_smoothing_real, device=real.device)\n","        real_labels[flip_real] = label_smoothing_fake\n","\n","        flip_fake = torch.rand(batch_size, device=real.device) < 0.1\n","        fake_labels = torch.full((batch_size,), label_smoothing_fake, device=real.device)\n","        fake_labels[flip_fake] = label_smoothing_real\n","\n","        disc_real = discriminator(real, labels).view(-1)\n","        loss_real = criterion(disc_real, real_labels)\n","\n","        disc_fake = discriminator(fake.detach(), labels).view(-1)\n","        loss_fake = criterion(disc_fake, fake_labels)\n","\n","        loss_disc = (loss_real + loss_fake) / 2\n","\n","        # Optimize discriminator\n","        optim_disc.zero_grad()\n","        loss_disc.backward()\n","        optim_disc.step()\n","\n","        # Train Generator\n","        for _ in range(2):  # Train generator twice for every discriminator step\n","            noise = torch.randn(batch_size, noise_dim, 1, 1).to(\"cuda\")\n","            fake = generator(noise, labels)\n","            disc_fake = discriminator(fake, labels).view(-1)\n","\n","            loss_gen = criterion(disc_fake, torch.ones_like(disc_fake))\n","\n","            optim_gen.zero_grad()\n","            loss_gen.backward()\n","            optim_gen.step()\n","\n","        # Update progress bar with losses\n","        pbar.set_postfix({\"Loss D\": loss_disc.item(), \"Loss G\": loss_gen.item()})\n","\n","    # Track losses\n","    losses_gen.append(loss_gen.item())\n","    losses_disc.append(loss_disc.item())\n","\n","    # Print progress\n","    print(f\"Epoch [{epoch+1}/{epochs}] Loss D: {loss_disc:.4f}, Loss G: {loss_gen:.4f}\")\n","\n","    # Visualize real and generated samples every 10 epochs\n","    if (epoch + 1) % 1 == 0:\n","        visualize_real_and_generated(dataset, generator, epoch + 1, num_classes, noise_dim)\n","\n","# Save the final models\n","torch.save(generator.state_dict(), \"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/models/enhanced_generator.pth\")\n","torch.save(discriminator.state_dict(), \"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/models/enhanced_discriminator.pth\")"]},{"cell_type":"code","source":["# Plot learning curves\n","plt.figure(figsize=(10, 5))\n","plt.plot(losses_gen, label='Generator Loss')\n","plt.plot(losses_disc, label='Discriminator Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.title('Learning Curve')\n","plt.show()\n","plt.savefig(\"/content/drive/MyDrive/Artificial_Intelligence_Course_NTUT/hw3a/output/enhanced_learning_curve.png\")\n","plt.close()"],"metadata":{"id":"TQIqjAQp2DAp","executionInfo":{"status":"aborted","timestamp":1734499587508,"user_tz":-480,"elapsed":4,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"}}},"execution_count":null,"outputs":[]}]}